{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1     1193       5  978300760\n",
       "1       1      661       3  978302109\n",
       "2       1      914       3  978301968\n",
       "3       1     3408       4  978300275\n",
       "4       1     2355       5  978824291"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "df = pd.read_csv(\"ratings.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movieIdCat</th>\n",
       "      <th>userIdCat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>1104</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>639</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>853</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>3177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>2162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp  movieIdCat  userIdCat\n",
       "0       1     1193       5  978300760        1104          0\n",
       "1       1      661       3  978302109         639          0\n",
       "2       1      914       3  978301968         853          0\n",
       "3       1     3408       4  978300275        3177          0\n",
       "4       1     2355       5  978824291        2162          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['movieIdCat'] = df.movieId.astype('category').cat.codes.values\n",
    "df['userIdCat'] = df.userId.astype('category').cat.codes.values\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df.groupby('userId').apply(lambda x: x.sample(frac=0.8, random_state = 0)).index.get_level_values(1)\n",
    "\n",
    "train = df.iloc[idx, :].reset_index(drop = True)\n",
    "test = df.drop(idx).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users, n_movies = len(df.userIdCat.unique()), len(df.movieIdCat.unique())\n",
    "n_latent_factors = 3\n",
    "n_latent_factors_user = 5\n",
    "n_latent_factors_movie = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, n_movies, n_latent_factors_movie, n_users, n_latent_factors_user):\n",
    "        super().__init__()\n",
    "        self.embedding_m = nn.Embedding(n_movies+1, n_latent_factors_movie)\n",
    "        self.embedding_u = nn.Embedding(n_users+1, n_latent_factors_user)\n",
    "\n",
    "        self.lin1 = nn.Linear(13, 200)\n",
    "        self.lin2 = nn.Linear(200, 80)\n",
    "        self.lin3 = nn.Linear(80, 50)\n",
    "        self.lin4 = nn.Linear(50, 20)\n",
    "        self.lin5 = nn.Linear(20, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn. Dropout(0.1)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        m_emb = self.embedding_m(x)\n",
    "        u_emb = self.embedding_u(y)\n",
    "        com = torch.cat((m_emb, u_emb), 1)\n",
    "        res = self.lin1(com)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout(res)\n",
    "        res = self.lin2(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout(res)\n",
    "        res = self.lin3(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout(res)\n",
    "        res = self.lin4(res)\n",
    "        res = self.relu(res)\n",
    "        res = self.dropout(res)\n",
    "        res = self.lin5(res)\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pickgpu 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 3.766148805618286\n",
      "Epoch 1: train loss: 3.7354164123535156\n",
      "Epoch 2: train loss: 3.700232744216919\n",
      "Epoch 3: train loss: 3.659844160079956\n",
      "Epoch 4: train loss: 3.6135575771331787\n",
      "Epoch 5: train loss: 3.5601823329925537\n",
      "Epoch 6: train loss: 3.498457193374634\n",
      "Epoch 7: train loss: 3.427135705947876\n",
      "Epoch 8: train loss: 3.3451426029205322\n",
      "Epoch 9: train loss: 3.250765562057495\n",
      "Epoch 10: train loss: 3.141899347305298\n",
      "Epoch 11: train loss: 3.01674222946167\n",
      "Epoch 12: train loss: 2.8740997314453125\n",
      "Epoch 13: train loss: 2.713160753250122\n",
      "Epoch 14: train loss: 2.5338220596313477\n",
      "Epoch 15: train loss: 2.3349387645721436\n",
      "Epoch 16: train loss: 2.117828845977783\n",
      "Epoch 17: train loss: 1.887163758277893\n",
      "Epoch 18: train loss: 1.6518291234970093\n",
      "Epoch 19: train loss: 1.431620478630066\n",
      "Epoch 20: train loss: 1.258760929107666\n",
      "Epoch 21: train loss: 1.172308087348938\n",
      "Epoch 22: train loss: 1.1952606439590454\n",
      "Epoch 23: train loss: 1.3016220331192017\n",
      "Epoch 24: train loss: 1.4256922006607056\n",
      "Epoch 25: train loss: 1.4955452680587769\n",
      "Epoch 26: train loss: 1.501565933227539\n",
      "Epoch 27: train loss: 1.448441505432129\n",
      "Epoch 28: train loss: 1.3664275407791138\n",
      "Epoch 29: train loss: 1.2763173580169678\n",
      "Epoch 30: train loss: 1.2042572498321533\n",
      "Epoch 31: train loss: 1.1586267948150635\n",
      "Epoch 32: train loss: 1.1443941593170166\n",
      "Epoch 33: train loss: 1.1532952785491943\n",
      "Epoch 34: train loss: 1.1746071577072144\n",
      "Epoch 35: train loss: 1.20052170753479\n",
      "Epoch 36: train loss: 1.2204760313034058\n",
      "Epoch 37: train loss: 1.2314207553863525\n",
      "Epoch 38: train loss: 1.2320891618728638\n",
      "Epoch 39: train loss: 1.2228567600250244\n",
      "Epoch 40: train loss: 1.205285668373108\n",
      "Epoch 41: train loss: 1.1839251518249512\n",
      "Epoch 42: train loss: 1.16216242313385\n",
      "Epoch 43: train loss: 1.1435387134552002\n",
      "Epoch 44: train loss: 1.131840467453003\n",
      "Epoch 45: train loss: 1.1264127492904663\n",
      "Epoch 46: train loss: 1.1285516023635864\n",
      "Epoch 47: train loss: 1.1361830234527588\n",
      "Epoch 48: train loss: 1.143825888633728\n",
      "Epoch 49: train loss: 1.1498539447784424\n",
      "Epoch 50: train loss: 1.1503123044967651\n",
      "Epoch 51: train loss: 1.1481462717056274\n",
      "Epoch 52: train loss: 1.140681505203247\n",
      "Epoch 53: train loss: 1.1320691108703613\n",
      "Epoch 54: train loss: 1.1253960132598877\n",
      "Epoch 55: train loss: 1.119303822517395\n",
      "Epoch 56: train loss: 1.117961049079895\n",
      "Epoch 57: train loss: 1.1169942617416382\n",
      "Epoch 58: train loss: 1.1187336444854736\n",
      "Epoch 59: train loss: 1.1210646629333496\n",
      "Epoch 60: train loss: 1.122038722038269\n",
      "Epoch 61: train loss: 1.1222590208053589\n",
      "Epoch 62: train loss: 1.1201825141906738\n",
      "Epoch 63: train loss: 1.1175200939178467\n",
      "Epoch 64: train loss: 1.1145265102386475\n",
      "Epoch 65: train loss: 1.1102148294448853\n",
      "Epoch 66: train loss: 1.109400749206543\n",
      "Epoch 67: train loss: 1.1075929403305054\n",
      "Epoch 68: train loss: 1.10599946975708\n",
      "Epoch 69: train loss: 1.1050400733947754\n",
      "Epoch 70: train loss: 1.106943964958191\n",
      "Epoch 71: train loss: 1.1067771911621094\n",
      "Epoch 72: train loss: 1.1053850650787354\n",
      "Epoch 73: train loss: 1.1036152839660645\n",
      "Epoch 74: train loss: 1.101957082748413\n",
      "Epoch 75: train loss: 1.1004770994186401\n",
      "Epoch 76: train loss: 1.0989550352096558\n",
      "Epoch 77: train loss: 1.0974915027618408\n",
      "Epoch 78: train loss: 1.0970144271850586\n",
      "Epoch 79: train loss: 1.0959049463272095\n",
      "Epoch 80: train loss: 1.0960034132003784\n",
      "Epoch 81: train loss: 1.0952762365341187\n",
      "Epoch 82: train loss: 1.09367835521698\n",
      "Epoch 83: train loss: 1.0935263633728027\n",
      "Epoch 84: train loss: 1.0913474559783936\n",
      "Epoch 85: train loss: 1.0903111696243286\n",
      "Epoch 86: train loss: 1.090727686882019\n",
      "Epoch 87: train loss: 1.0884552001953125\n",
      "Epoch 88: train loss: 1.0877817869186401\n",
      "Epoch 89: train loss: 1.0872153043746948\n",
      "Epoch 90: train loss: 1.0863052606582642\n",
      "Epoch 91: train loss: 1.086242437362671\n",
      "Epoch 92: train loss: 1.0835697650909424\n",
      "Epoch 93: train loss: 1.0836313962936401\n",
      "Epoch 94: train loss: 1.082466721534729\n",
      "Epoch 95: train loss: 1.0805245637893677\n",
      "Epoch 96: train loss: 1.080199122428894\n",
      "Epoch 97: train loss: 1.0795329809188843\n",
      "Epoch 98: train loss: 1.078625202178955\n",
      "Epoch 99: train loss: 1.0768481492996216\n",
      "Epoch 100: train loss: 1.0773473978042603\n",
      "Epoch 101: train loss: 1.0751380920410156\n",
      "Epoch 102: train loss: 1.0748165845870972\n",
      "Epoch 103: train loss: 1.074119210243225\n",
      "Epoch 104: train loss: 1.0733203887939453\n",
      "Epoch 105: train loss: 1.070743203163147\n",
      "Epoch 106: train loss: 1.0710734128952026\n",
      "Epoch 107: train loss: 1.070731520652771\n",
      "Epoch 108: train loss: 1.068644642829895\n",
      "Epoch 109: train loss: 1.0674773454666138\n",
      "Epoch 110: train loss: 1.0669466257095337\n",
      "Epoch 111: train loss: 1.0658453702926636\n",
      "Epoch 112: train loss: 1.0644270181655884\n",
      "Epoch 113: train loss: 1.0641387701034546\n",
      "Epoch 114: train loss: 1.0621774196624756\n",
      "Epoch 115: train loss: 1.0612753629684448\n",
      "Epoch 116: train loss: 1.0606164932250977\n",
      "Epoch 117: train loss: 1.0582900047302246\n",
      "Epoch 118: train loss: 1.0573854446411133\n",
      "Epoch 119: train loss: 1.0562535524368286\n",
      "Epoch 120: train loss: 1.0550994873046875\n",
      "Epoch 121: train loss: 1.0532448291778564\n",
      "Epoch 122: train loss: 1.0530672073364258\n",
      "Epoch 123: train loss: 1.051486611366272\n",
      "Epoch 124: train loss: 1.0506775379180908\n",
      "Epoch 125: train loss: 1.0495132207870483\n",
      "Epoch 126: train loss: 1.047805666923523\n",
      "Epoch 127: train loss: 1.0462754964828491\n",
      "Epoch 128: train loss: 1.0442328453063965\n",
      "Epoch 129: train loss: 1.0437103509902954\n",
      "Epoch 130: train loss: 1.0419872999191284\n",
      "Epoch 131: train loss: 1.0403811931610107\n",
      "Epoch 132: train loss: 1.0389416217803955\n",
      "Epoch 133: train loss: 1.0380727052688599\n",
      "Epoch 134: train loss: 1.0358737707138062\n",
      "Epoch 135: train loss: 1.0361326932907104\n",
      "Epoch 136: train loss: 1.03389573097229\n",
      "Epoch 137: train loss: 1.032978892326355\n",
      "Epoch 138: train loss: 1.0313528776168823\n",
      "Epoch 139: train loss: 1.0294809341430664\n",
      "Epoch 140: train loss: 1.0292961597442627\n",
      "Epoch 141: train loss: 1.0280861854553223\n",
      "Epoch 142: train loss: 1.0267698764801025\n",
      "Epoch 143: train loss: 1.0261967182159424\n",
      "Epoch 144: train loss: 1.0245689153671265\n",
      "Epoch 145: train loss: 1.0239448547363281\n",
      "Epoch 146: train loss: 1.0219624042510986\n",
      "Epoch 147: train loss: 1.0216178894042969\n",
      "Epoch 148: train loss: 1.0204132795333862\n",
      "Epoch 149: train loss: 1.0201880931854248\n",
      "Epoch 150: train loss: 1.0188908576965332\n",
      "Epoch 151: train loss: 1.0172381401062012\n",
      "Epoch 152: train loss: 1.0176024436950684\n",
      "Epoch 153: train loss: 1.016222357749939\n",
      "Epoch 154: train loss: 1.016141653060913\n",
      "Epoch 155: train loss: 1.0151807069778442\n",
      "Epoch 156: train loss: 1.0138386487960815\n",
      "Epoch 157: train loss: 1.0134230852127075\n",
      "Epoch 158: train loss: 1.0138667821884155\n",
      "Epoch 159: train loss: 1.0120511054992676\n",
      "Epoch 160: train loss: 1.0119144916534424\n",
      "Epoch 161: train loss: 1.009989619255066\n",
      "Epoch 162: train loss: 1.0109933614730835\n",
      "Epoch 163: train loss: 1.0103651285171509\n",
      "Epoch 164: train loss: 1.0089720487594604\n",
      "Epoch 165: train loss: 1.0083884000778198\n",
      "Epoch 166: train loss: 1.0078034400939941\n",
      "Epoch 167: train loss: 1.007156252861023\n",
      "Epoch 168: train loss: 1.0060752630233765\n",
      "Epoch 169: train loss: 1.0060629844665527\n",
      "Epoch 170: train loss: 1.0055028200149536\n",
      "Epoch 171: train loss: 1.0055086612701416\n",
      "Epoch 172: train loss: 1.0037542581558228\n",
      "Epoch 173: train loss: 1.0030845403671265\n",
      "Epoch 174: train loss: 1.0034841299057007\n",
      "Epoch 175: train loss: 1.0036951303482056\n",
      "Epoch 176: train loss: 1.0024818181991577\n",
      "Epoch 177: train loss: 1.0021746158599854\n",
      "Epoch 178: train loss: 1.0014121532440186\n",
      "Epoch 179: train loss: 1.0010422468185425\n",
      "Epoch 180: train loss: 1.0003035068511963\n",
      "Epoch 181: train loss: 1.0004277229309082\n",
      "Epoch 182: train loss: 0.9991403818130493\n",
      "Epoch 183: train loss: 0.9988903403282166\n",
      "Epoch 184: train loss: 0.9979281425476074\n",
      "Epoch 185: train loss: 0.9977515339851379\n",
      "Epoch 186: train loss: 0.9971775412559509\n",
      "Epoch 187: train loss: 0.9960153102874756\n",
      "Epoch 188: train loss: 0.997026801109314\n",
      "Epoch 189: train loss: 0.9961225390434265\n",
      "Epoch 190: train loss: 0.9968550801277161\n",
      "Epoch 191: train loss: 0.9954720735549927\n",
      "Epoch 192: train loss: 0.9950680136680603\n",
      "Epoch 193: train loss: 0.9940195083618164\n",
      "Epoch 194: train loss: 0.9939318895339966\n",
      "Epoch 195: train loss: 0.9938467144966125\n",
      "Epoch 196: train loss: 0.9924840331077576\n",
      "Epoch 197: train loss: 0.9926775097846985\n",
      "Epoch 198: train loss: 0.992021918296814\n",
      "Epoch 199: train loss: 0.9920111298561096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: train loss: 0.9910435080528259\n",
      "Epoch 201: train loss: 0.9903470873832703\n",
      "Epoch 202: train loss: 0.990023136138916\n",
      "Epoch 203: train loss: 0.99049973487854\n",
      "Epoch 204: train loss: 0.9898866415023804\n",
      "Epoch 205: train loss: 0.9893985986709595\n",
      "Epoch 206: train loss: 0.9888726472854614\n",
      "Epoch 207: train loss: 0.9885883331298828\n",
      "Epoch 208: train loss: 0.9880170822143555\n",
      "Epoch 209: train loss: 0.9878482222557068\n",
      "Epoch 210: train loss: 0.9882546663284302\n",
      "Epoch 211: train loss: 0.9872698187828064\n",
      "Epoch 212: train loss: 0.9876919984817505\n",
      "Epoch 213: train loss: 0.9855415225028992\n",
      "Epoch 214: train loss: 0.9863256216049194\n",
      "Epoch 215: train loss: 0.985617995262146\n",
      "Epoch 216: train loss: 0.9854127168655396\n",
      "Epoch 217: train loss: 0.9858022928237915\n",
      "Epoch 218: train loss: 0.9849465489387512\n",
      "Epoch 219: train loss: 0.9842333793640137\n",
      "Epoch 220: train loss: 0.9842937588691711\n",
      "Epoch 221: train loss: 0.9839208722114563\n",
      "Epoch 222: train loss: 0.9838982224464417\n",
      "Epoch 223: train loss: 0.9823727607727051\n",
      "Epoch 224: train loss: 0.9827236533164978\n",
      "Epoch 225: train loss: 0.9814969301223755\n",
      "Epoch 226: train loss: 0.981198251247406\n",
      "Epoch 227: train loss: 0.9820663928985596\n",
      "Epoch 228: train loss: 0.9827691316604614\n",
      "Epoch 229: train loss: 0.9820504784584045\n",
      "Epoch 230: train loss: 0.9810023903846741\n",
      "Epoch 231: train loss: 0.9810225963592529\n",
      "Epoch 232: train loss: 0.9810395240783691\n",
      "Epoch 233: train loss: 0.9806782007217407\n",
      "Epoch 234: train loss: 0.9798681735992432\n",
      "Epoch 235: train loss: 0.9799758195877075\n",
      "Epoch 236: train loss: 0.979152500629425\n",
      "Epoch 237: train loss: 0.9794042110443115\n",
      "Epoch 238: train loss: 0.9798399806022644\n",
      "Epoch 239: train loss: 0.9783951044082642\n",
      "Epoch 240: train loss: 0.9782702326774597\n",
      "Epoch 241: train loss: 0.9783317446708679\n",
      "Epoch 242: train loss: 0.9771682620048523\n",
      "Epoch 243: train loss: 0.977362334728241\n",
      "Epoch 244: train loss: 0.9780313372612\n",
      "Epoch 245: train loss: 0.977754533290863\n",
      "Epoch 246: train loss: 0.9773485064506531\n",
      "Epoch 247: train loss: 0.9766011834144592\n",
      "Epoch 248: train loss: 0.9763156175613403\n",
      "Epoch 249: train loss: 0.9758903980255127\n",
      "Epoch 250: train loss: 0.9751982688903809\n",
      "Epoch 251: train loss: 0.9761424660682678\n",
      "Epoch 252: train loss: 0.9758976697921753\n",
      "Epoch 253: train loss: 0.9756389856338501\n",
      "Epoch 254: train loss: 0.9752501845359802\n",
      "Epoch 255: train loss: 0.9749243259429932\n",
      "Epoch 256: train loss: 0.9753285646438599\n",
      "Epoch 257: train loss: 0.9743127226829529\n",
      "Epoch 258: train loss: 0.97425377368927\n",
      "Epoch 259: train loss: 0.9732560515403748\n",
      "Epoch 260: train loss: 0.973199725151062\n",
      "Epoch 261: train loss: 0.9726859331130981\n",
      "Epoch 262: train loss: 0.9729689359664917\n",
      "Epoch 263: train loss: 0.972884476184845\n",
      "Epoch 264: train loss: 0.9728192090988159\n",
      "Epoch 265: train loss: 0.9730357527732849\n",
      "Epoch 266: train loss: 0.9719351530075073\n",
      "Epoch 267: train loss: 0.9714863896369934\n",
      "Epoch 268: train loss: 0.9707779288291931\n",
      "Epoch 269: train loss: 0.9711783528327942\n",
      "Epoch 270: train loss: 0.9709857106208801\n",
      "Epoch 271: train loss: 0.9696075916290283\n",
      "Epoch 272: train loss: 0.9713134765625\n",
      "Epoch 273: train loss: 0.9698372483253479\n",
      "Epoch 274: train loss: 0.9693953990936279\n",
      "Epoch 275: train loss: 0.9687278866767883\n",
      "Epoch 276: train loss: 0.9684776663780212\n",
      "Epoch 277: train loss: 0.9687011241912842\n",
      "Epoch 278: train loss: 0.9672837853431702\n",
      "Epoch 279: train loss: 0.9681574702262878\n",
      "Epoch 280: train loss: 0.9675220251083374\n",
      "Epoch 281: train loss: 0.9661582708358765\n",
      "Epoch 282: train loss: 0.9658580422401428\n",
      "Epoch 283: train loss: 0.966042697429657\n",
      "Epoch 284: train loss: 0.9648799896240234\n",
      "Epoch 285: train loss: 0.9636499285697937\n",
      "Epoch 286: train loss: 0.9640911221504211\n",
      "Epoch 287: train loss: 0.9640260934829712\n",
      "Epoch 288: train loss: 0.9629258513450623\n",
      "Epoch 289: train loss: 0.9623352289199829\n",
      "Epoch 290: train loss: 0.9620609283447266\n",
      "Epoch 291: train loss: 0.9618868231773376\n",
      "Epoch 292: train loss: 0.9604945182800293\n",
      "Epoch 293: train loss: 0.9613213539123535\n",
      "Epoch 294: train loss: 0.9607323408126831\n",
      "Epoch 295: train loss: 0.959971010684967\n",
      "Epoch 296: train loss: 0.9595801830291748\n",
      "Epoch 297: train loss: 0.9596047401428223\n",
      "Epoch 298: train loss: 0.9590473771095276\n",
      "Epoch 299: train loss: 0.9588308334350586\n",
      "Epoch 300: train loss: 0.957917332649231\n",
      "Epoch 301: train loss: 0.9579058885574341\n",
      "Epoch 302: train loss: 0.9574667811393738\n",
      "Epoch 303: train loss: 0.9580923318862915\n",
      "Epoch 304: train loss: 0.9583056569099426\n",
      "Epoch 305: train loss: 0.9580017328262329\n",
      "Epoch 306: train loss: 0.9576632380485535\n",
      "Epoch 307: train loss: 0.9564207792282104\n",
      "Epoch 308: train loss: 0.9562480449676514\n",
      "Epoch 309: train loss: 0.95524662733078\n",
      "Epoch 310: train loss: 0.9559972882270813\n",
      "Epoch 311: train loss: 0.9555349349975586\n",
      "Epoch 312: train loss: 0.9567497968673706\n",
      "Epoch 313: train loss: 0.9551670551300049\n",
      "Epoch 314: train loss: 0.9550735950469971\n",
      "Epoch 315: train loss: 0.9547463059425354\n",
      "Epoch 316: train loss: 0.9537157416343689\n",
      "Epoch 317: train loss: 0.953978419303894\n",
      "Epoch 318: train loss: 0.9541172981262207\n",
      "Epoch 319: train loss: 0.9533565640449524\n",
      "Epoch 320: train loss: 0.953834056854248\n",
      "Epoch 321: train loss: 0.9532104730606079\n",
      "Epoch 322: train loss: 0.9528833031654358\n",
      "Epoch 323: train loss: 0.953577995300293\n",
      "Epoch 324: train loss: 0.9521636962890625\n",
      "Epoch 325: train loss: 0.9519743919372559\n",
      "Epoch 326: train loss: 0.9524238109588623\n",
      "Epoch 327: train loss: 0.9520114064216614\n",
      "Epoch 328: train loss: 0.951576828956604\n",
      "Epoch 329: train loss: 0.9511498808860779\n",
      "Epoch 330: train loss: 0.9504984021186829\n",
      "Epoch 331: train loss: 0.9505787491798401\n",
      "Epoch 332: train loss: 0.9513798356056213\n",
      "Epoch 333: train loss: 0.9507704973220825\n",
      "Epoch 334: train loss: 0.9500004649162292\n",
      "Epoch 335: train loss: 0.9495791792869568\n",
      "Epoch 336: train loss: 0.9505080580711365\n",
      "Epoch 337: train loss: 0.9500901103019714\n",
      "Epoch 338: train loss: 0.9489601254463196\n",
      "Epoch 339: train loss: 0.9499178528785706\n",
      "Epoch 340: train loss: 0.94878751039505\n",
      "Epoch 341: train loss: 0.948588490486145\n",
      "Epoch 342: train loss: 0.9484302401542664\n",
      "Epoch 343: train loss: 0.9492093324661255\n",
      "Epoch 344: train loss: 0.9474924802780151\n",
      "Epoch 345: train loss: 0.9475221633911133\n",
      "Epoch 346: train loss: 0.9476895332336426\n",
      "Epoch 347: train loss: 0.9469678997993469\n",
      "Epoch 348: train loss: 0.9466125965118408\n",
      "Epoch 349: train loss: 0.9466123580932617\n",
      "Epoch 350: train loss: 0.9461502432823181\n",
      "Epoch 351: train loss: 0.9460679888725281\n",
      "Epoch 352: train loss: 0.9460023641586304\n",
      "Epoch 353: train loss: 0.9450468420982361\n",
      "Epoch 354: train loss: 0.9456098079681396\n",
      "Epoch 355: train loss: 0.945889413356781\n",
      "Epoch 356: train loss: 0.9458888173103333\n",
      "Epoch 357: train loss: 0.9444493055343628\n",
      "Epoch 358: train loss: 0.94480961561203\n",
      "Epoch 359: train loss: 0.9441295266151428\n",
      "Epoch 360: train loss: 0.9445942044258118\n",
      "Epoch 361: train loss: 0.9430291056632996\n",
      "Epoch 362: train loss: 0.943398654460907\n",
      "Epoch 363: train loss: 0.9434138536453247\n",
      "Epoch 364: train loss: 0.9435552954673767\n",
      "Epoch 365: train loss: 0.9435306787490845\n",
      "Epoch 366: train loss: 0.9425172209739685\n",
      "Epoch 367: train loss: 0.9419291615486145\n",
      "Epoch 368: train loss: 0.942010760307312\n",
      "Epoch 369: train loss: 0.9420284628868103\n",
      "Epoch 370: train loss: 0.941397488117218\n",
      "Epoch 371: train loss: 0.9419096112251282\n",
      "Epoch 372: train loss: 0.9416078329086304\n",
      "Epoch 373: train loss: 0.9402323365211487\n",
      "Epoch 374: train loss: 0.9405333995819092\n",
      "Epoch 375: train loss: 0.9412388205528259\n",
      "Epoch 376: train loss: 0.939473032951355\n",
      "Epoch 377: train loss: 0.9403291940689087\n",
      "Epoch 378: train loss: 0.9385441541671753\n",
      "Epoch 379: train loss: 0.9387774467468262\n",
      "Epoch 380: train loss: 0.9393882751464844\n",
      "Epoch 381: train loss: 0.9392707347869873\n",
      "Epoch 382: train loss: 0.9387580156326294\n",
      "Epoch 383: train loss: 0.937998354434967\n",
      "Epoch 384: train loss: 0.937725305557251\n",
      "Epoch 385: train loss: 0.9380583763122559\n",
      "Epoch 386: train loss: 0.9379386901855469\n",
      "Epoch 387: train loss: 0.9371873736381531\n",
      "Epoch 388: train loss: 0.9364545941352844\n",
      "Epoch 389: train loss: 0.9368739724159241\n",
      "Epoch 390: train loss: 0.9368827939033508\n",
      "Epoch 391: train loss: 0.9362441301345825\n",
      "Epoch 392: train loss: 0.9358201622962952\n",
      "Epoch 393: train loss: 0.9354053139686584\n",
      "Epoch 394: train loss: 0.9353501200675964\n",
      "Epoch 395: train loss: 0.9354042410850525\n",
      "Epoch 396: train loss: 0.9349067807197571\n",
      "Epoch 397: train loss: 0.9347618222236633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 398: train loss: 0.9346501231193542\n",
      "Epoch 399: train loss: 0.9341038465499878\n",
      "Epoch 400: train loss: 0.9338788986206055\n",
      "Epoch 401: train loss: 0.9339330792427063\n",
      "Epoch 402: train loss: 0.9335679411888123\n",
      "Epoch 403: train loss: 0.9328808784484863\n",
      "Epoch 404: train loss: 0.9326488971710205\n",
      "Epoch 405: train loss: 0.9324449896812439\n",
      "Epoch 406: train loss: 0.9324544668197632\n",
      "Epoch 407: train loss: 0.9315348267555237\n",
      "Epoch 408: train loss: 0.9317287802696228\n",
      "Epoch 409: train loss: 0.9310141205787659\n",
      "Epoch 410: train loss: 0.9315198659896851\n",
      "Epoch 411: train loss: 0.9311258792877197\n",
      "Epoch 412: train loss: 0.9306558966636658\n",
      "Epoch 413: train loss: 0.9301113486289978\n",
      "Epoch 414: train loss: 0.9296546578407288\n",
      "Epoch 415: train loss: 0.9301071166992188\n",
      "Epoch 416: train loss: 0.9292166829109192\n",
      "Epoch 417: train loss: 0.9297637939453125\n",
      "Epoch 418: train loss: 0.9287779331207275\n",
      "Epoch 419: train loss: 0.9287789463996887\n",
      "Epoch 420: train loss: 0.928575336933136\n",
      "Epoch 421: train loss: 0.9275963306427002\n",
      "Epoch 422: train loss: 0.9281785488128662\n",
      "Epoch 423: train loss: 0.9271965026855469\n",
      "Epoch 424: train loss: 0.9281194806098938\n",
      "Epoch 425: train loss: 0.927127480506897\n",
      "Epoch 426: train loss: 0.9274216890335083\n",
      "Epoch 427: train loss: 0.9273101091384888\n",
      "Epoch 428: train loss: 0.9269013404846191\n",
      "Epoch 429: train loss: 0.925961434841156\n",
      "Epoch 430: train loss: 0.9251337647438049\n",
      "Epoch 431: train loss: 0.925621509552002\n",
      "Epoch 432: train loss: 0.9246374368667603\n",
      "Epoch 433: train loss: 0.9242724776268005\n",
      "Epoch 434: train loss: 0.9248688817024231\n",
      "Epoch 435: train loss: 0.9249793291091919\n",
      "Epoch 436: train loss: 0.9229419827461243\n",
      "Epoch 437: train loss: 0.9242571592330933\n",
      "Epoch 438: train loss: 0.923800528049469\n",
      "Epoch 439: train loss: 0.923159658908844\n",
      "Epoch 440: train loss: 0.9233630895614624\n",
      "Epoch 441: train loss: 0.9226928353309631\n",
      "Epoch 442: train loss: 0.922360897064209\n",
      "Epoch 443: train loss: 0.9216327667236328\n",
      "Epoch 444: train loss: 0.9216616749763489\n",
      "Epoch 445: train loss: 0.9216688871383667\n",
      "Epoch 446: train loss: 0.9213607907295227\n",
      "Epoch 447: train loss: 0.9207797050476074\n",
      "Epoch 448: train loss: 0.9210094809532166\n",
      "Epoch 449: train loss: 0.9205924272537231\n",
      "Epoch 450: train loss: 0.9206087589263916\n",
      "Epoch 451: train loss: 0.920083224773407\n",
      "Epoch 452: train loss: 0.9197940230369568\n",
      "Epoch 453: train loss: 0.9192923903465271\n",
      "Epoch 454: train loss: 0.9183262586593628\n",
      "Epoch 455: train loss: 0.9185205698013306\n",
      "Epoch 456: train loss: 0.9191551208496094\n",
      "Epoch 457: train loss: 0.9179161787033081\n",
      "Epoch 458: train loss: 0.9179818630218506\n",
      "Epoch 459: train loss: 0.9174438118934631\n",
      "Epoch 460: train loss: 0.9161332249641418\n",
      "Epoch 461: train loss: 0.9166029095649719\n",
      "Epoch 462: train loss: 0.9153460264205933\n",
      "Epoch 463: train loss: 0.9159882068634033\n",
      "Epoch 464: train loss: 0.9166857004165649\n",
      "Epoch 465: train loss: 0.9152660369873047\n",
      "Epoch 466: train loss: 0.9155628681182861\n",
      "Epoch 467: train loss: 0.9148291349411011\n",
      "Epoch 468: train loss: 0.915073037147522\n",
      "Epoch 469: train loss: 0.9151266813278198\n",
      "Epoch 470: train loss: 0.9137226343154907\n",
      "Epoch 471: train loss: 0.9140574336051941\n",
      "Epoch 472: train loss: 0.9135356545448303\n",
      "Epoch 473: train loss: 0.9135562777519226\n",
      "Epoch 474: train loss: 0.9133516550064087\n",
      "Epoch 475: train loss: 0.9135051965713501\n",
      "Epoch 476: train loss: 0.9125674962997437\n",
      "Epoch 477: train loss: 0.9120502471923828\n",
      "Epoch 478: train loss: 0.9125977754592896\n",
      "Epoch 479: train loss: 0.9114416241645813\n",
      "Epoch 480: train loss: 0.9112840890884399\n",
      "Epoch 481: train loss: 0.9110616445541382\n",
      "Epoch 482: train loss: 0.9114429950714111\n",
      "Epoch 483: train loss: 0.9111905097961426\n",
      "Epoch 484: train loss: 0.910169243812561\n",
      "Epoch 485: train loss: 0.9102408289909363\n",
      "Epoch 486: train loss: 0.910812258720398\n",
      "Epoch 487: train loss: 0.9092168211936951\n",
      "Epoch 488: train loss: 0.9090364575386047\n",
      "Epoch 489: train loss: 0.9093809127807617\n",
      "Epoch 490: train loss: 0.9087516665458679\n",
      "Epoch 491: train loss: 0.908886194229126\n",
      "Epoch 492: train loss: 0.9082556962966919\n",
      "Epoch 493: train loss: 0.9088122844696045\n",
      "Epoch 494: train loss: 0.9077917337417603\n",
      "Epoch 495: train loss: 0.9077092409133911\n",
      "Epoch 496: train loss: 0.9077939391136169\n",
      "Epoch 497: train loss: 0.9061009287834167\n",
      "Epoch 498: train loss: 0.9060842990875244\n",
      "Epoch 499: train loss: 0.9065566062927246\n",
      "Epoch 500: train loss: 0.9056822061538696\n",
      "Epoch 501: train loss: 0.9070358276367188\n",
      "Epoch 502: train loss: 0.9057320356369019\n",
      "Epoch 503: train loss: 0.9054019451141357\n",
      "Epoch 504: train loss: 0.9043117165565491\n",
      "Epoch 505: train loss: 0.9064910411834717\n",
      "Epoch 506: train loss: 0.904949963092804\n",
      "Epoch 507: train loss: 0.9039624929428101\n",
      "Epoch 508: train loss: 0.9043406248092651\n",
      "Epoch 509: train loss: 0.9046923518180847\n",
      "Epoch 510: train loss: 0.9038417339324951\n",
      "Epoch 511: train loss: 0.9034450054168701\n",
      "Epoch 512: train loss: 0.9037453532218933\n",
      "Epoch 513: train loss: 0.902664840221405\n",
      "Epoch 514: train loss: 0.9020752310752869\n",
      "Epoch 515: train loss: 0.9029815793037415\n",
      "Epoch 516: train loss: 0.9025341868400574\n",
      "Epoch 517: train loss: 0.9017556309700012\n",
      "Epoch 518: train loss: 0.9021220207214355\n",
      "Epoch 519: train loss: 0.9007629156112671\n",
      "Epoch 520: train loss: 0.9016851186752319\n",
      "Epoch 521: train loss: 0.9015064835548401\n",
      "Epoch 522: train loss: 0.9016450047492981\n",
      "Epoch 523: train loss: 0.9003384113311768\n",
      "Epoch 524: train loss: 0.9006187915802002\n",
      "Epoch 525: train loss: 0.9003546237945557\n",
      "Epoch 526: train loss: 0.8999531865119934\n",
      "Epoch 527: train loss: 0.9003702402114868\n",
      "Epoch 528: train loss: 0.8994765877723694\n",
      "Epoch 529: train loss: 0.8995042443275452\n",
      "Epoch 530: train loss: 0.8990700840950012\n",
      "Epoch 531: train loss: 0.8987234830856323\n",
      "Epoch 532: train loss: 0.8984912037849426\n",
      "Epoch 533: train loss: 0.8983840942382812\n",
      "Epoch 534: train loss: 0.8981886506080627\n",
      "Epoch 535: train loss: 0.8987882137298584\n",
      "Epoch 536: train loss: 0.8977771997451782\n",
      "Epoch 537: train loss: 0.8984870910644531\n",
      "Epoch 538: train loss: 0.8975156545639038\n",
      "Epoch 539: train loss: 0.8982231020927429\n",
      "Epoch 540: train loss: 0.8969665765762329\n",
      "Epoch 541: train loss: 0.8973031640052795\n",
      "Epoch 542: train loss: 0.8966473340988159\n",
      "Epoch 543: train loss: 0.8963937163352966\n",
      "Epoch 544: train loss: 0.8958696126937866\n",
      "Epoch 545: train loss: 0.8964647054672241\n",
      "Epoch 546: train loss: 0.8957204222679138\n",
      "Epoch 547: train loss: 0.8961865901947021\n",
      "Epoch 548: train loss: 0.8946799635887146\n",
      "Epoch 549: train loss: 0.895748496055603\n",
      "Epoch 550: train loss: 0.895301342010498\n",
      "Epoch 551: train loss: 0.894801676273346\n",
      "Epoch 552: train loss: 0.8948948383331299\n",
      "Epoch 553: train loss: 0.8946789503097534\n",
      "Epoch 554: train loss: 0.8939021229743958\n",
      "Epoch 555: train loss: 0.8935598134994507\n",
      "Epoch 556: train loss: 0.8933721780776978\n",
      "Epoch 557: train loss: 0.8935860395431519\n",
      "Epoch 558: train loss: 0.8937225937843323\n",
      "Epoch 559: train loss: 0.8935791850090027\n",
      "Epoch 560: train loss: 0.8927288055419922\n",
      "Epoch 561: train loss: 0.8935151696205139\n",
      "Epoch 562: train loss: 0.8924902081489563\n",
      "Epoch 563: train loss: 0.8926435112953186\n",
      "Epoch 564: train loss: 0.8932374715805054\n",
      "Epoch 565: train loss: 0.8911629319190979\n",
      "Epoch 566: train loss: 0.8910556435585022\n",
      "Epoch 567: train loss: 0.8917115926742554\n",
      "Epoch 568: train loss: 0.8910613059997559\n",
      "Epoch 569: train loss: 0.8916918039321899\n",
      "Epoch 570: train loss: 0.8913654088973999\n",
      "Epoch 571: train loss: 0.8914623260498047\n",
      "Epoch 572: train loss: 0.8906433582305908\n",
      "Epoch 573: train loss: 0.8902903199195862\n",
      "Epoch 574: train loss: 0.8916481137275696\n",
      "Epoch 575: train loss: 0.8906557559967041\n",
      "Epoch 576: train loss: 0.8898794054985046\n",
      "Epoch 577: train loss: 0.8897753357887268\n",
      "Epoch 578: train loss: 0.8892019987106323\n",
      "Epoch 579: train loss: 0.889604389667511\n",
      "Epoch 580: train loss: 0.8892810344696045\n",
      "Epoch 581: train loss: 0.889550507068634\n",
      "Epoch 582: train loss: 0.8888363242149353\n",
      "Epoch 583: train loss: 0.888533353805542\n",
      "Epoch 584: train loss: 0.8888587951660156\n",
      "Epoch 585: train loss: 0.8882315158843994\n",
      "Epoch 586: train loss: 0.8888667821884155\n",
      "Epoch 587: train loss: 0.8892381191253662\n",
      "Epoch 588: train loss: 0.8875302076339722\n",
      "Epoch 589: train loss: 0.8874204754829407\n",
      "Epoch 590: train loss: 0.8877776861190796\n",
      "Epoch 591: train loss: 0.8873295783996582\n",
      "Epoch 592: train loss: 0.8878461122512817\n",
      "Epoch 593: train loss: 0.8866302967071533\n",
      "Epoch 594: train loss: 0.886853039264679\n",
      "Epoch 595: train loss: 0.8861492276191711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 596: train loss: 0.8865623474121094\n",
      "Epoch 597: train loss: 0.8868972659111023\n",
      "Epoch 598: train loss: 0.8859747648239136\n",
      "Epoch 599: train loss: 0.8861514329910278\n",
      "Epoch 600: train loss: 0.8862234354019165\n",
      "Epoch 601: train loss: 0.8865970373153687\n",
      "Epoch 602: train loss: 0.8857471942901611\n",
      "Epoch 603: train loss: 0.885180652141571\n",
      "Epoch 604: train loss: 0.8859896063804626\n",
      "Epoch 605: train loss: 0.8848571181297302\n",
      "Epoch 606: train loss: 0.8852157592773438\n",
      "Epoch 607: train loss: 0.8842506408691406\n",
      "Epoch 608: train loss: 0.8840548992156982\n",
      "Epoch 609: train loss: 0.8838655948638916\n",
      "Epoch 610: train loss: 0.8833321928977966\n",
      "Epoch 611: train loss: 0.8841163516044617\n",
      "Epoch 612: train loss: 0.8833232522010803\n",
      "Epoch 613: train loss: 0.8837756514549255\n",
      "Epoch 614: train loss: 0.8831397294998169\n",
      "Epoch 615: train loss: 0.8828902840614319\n",
      "Epoch 616: train loss: 0.8837786316871643\n",
      "Epoch 617: train loss: 0.8828586935997009\n",
      "Epoch 618: train loss: 0.8819243907928467\n",
      "Epoch 619: train loss: 0.8818743824958801\n",
      "Epoch 620: train loss: 0.8826422095298767\n",
      "Epoch 621: train loss: 0.8819354176521301\n",
      "Epoch 622: train loss: 0.8823790550231934\n",
      "Epoch 623: train loss: 0.8821083903312683\n",
      "Epoch 624: train loss: 0.881702721118927\n",
      "Epoch 625: train loss: 0.8807503581047058\n",
      "Epoch 626: train loss: 0.8823151588439941\n",
      "Epoch 627: train loss: 0.8813164234161377\n",
      "Epoch 628: train loss: 0.8815363645553589\n",
      "Epoch 629: train loss: 0.8804777264595032\n",
      "Epoch 630: train loss: 0.8800539374351501\n",
      "Epoch 631: train loss: 0.8818469643592834\n",
      "Epoch 632: train loss: 0.8803800940513611\n",
      "Epoch 633: train loss: 0.8805332183837891\n",
      "Epoch 634: train loss: 0.8807825446128845\n",
      "Epoch 635: train loss: 0.8802946209907532\n",
      "Epoch 636: train loss: 0.8799206614494324\n",
      "Epoch 637: train loss: 0.8792287707328796\n",
      "Epoch 638: train loss: 0.8791232109069824\n",
      "Epoch 639: train loss: 0.8784716129302979\n",
      "Epoch 640: train loss: 0.8785634636878967\n",
      "Epoch 641: train loss: 0.8791182637214661\n",
      "Epoch 642: train loss: 0.8791513442993164\n",
      "Epoch 643: train loss: 0.8781670331954956\n",
      "Epoch 644: train loss: 0.878229558467865\n",
      "Epoch 645: train loss: 0.8778514862060547\n",
      "Epoch 646: train loss: 0.8782544136047363\n",
      "Epoch 647: train loss: 0.8782815337181091\n",
      "Epoch 648: train loss: 0.879039466381073\n",
      "Epoch 649: train loss: 0.8780125379562378\n",
      "Epoch 650: train loss: 0.8779686093330383\n",
      "Epoch 651: train loss: 0.87799471616745\n",
      "Epoch 652: train loss: 0.8778305649757385\n",
      "Epoch 653: train loss: 0.8776641488075256\n",
      "Epoch 654: train loss: 0.8772974014282227\n",
      "Epoch 655: train loss: 0.8773308992385864\n",
      "Epoch 656: train loss: 0.8773211240768433\n",
      "Epoch 657: train loss: 0.8772746324539185\n",
      "Epoch 658: train loss: 0.8765358328819275\n",
      "Epoch 659: train loss: 0.8761069178581238\n",
      "Epoch 660: train loss: 0.876511812210083\n",
      "Epoch 661: train loss: 0.8763063549995422\n",
      "Epoch 662: train loss: 0.8765717148780823\n",
      "Epoch 663: train loss: 0.8762261867523193\n",
      "Epoch 664: train loss: 0.8759324550628662\n",
      "Epoch 665: train loss: 0.8762189149856567\n",
      "Epoch 666: train loss: 0.8756847381591797\n",
      "Epoch 667: train loss: 0.8749881982803345\n",
      "Epoch 668: train loss: 0.8753936886787415\n",
      "Epoch 669: train loss: 0.8753123879432678\n",
      "Epoch 670: train loss: 0.8744093179702759\n",
      "Epoch 671: train loss: 0.8751268982887268\n",
      "Epoch 672: train loss: 0.874917209148407\n",
      "Epoch 673: train loss: 0.8744199275970459\n",
      "Epoch 674: train loss: 0.8749477863311768\n",
      "Epoch 675: train loss: 0.8741586208343506\n",
      "Epoch 676: train loss: 0.874126136302948\n",
      "Epoch 677: train loss: 0.8742469549179077\n",
      "Epoch 678: train loss: 0.8739379048347473\n",
      "Epoch 679: train loss: 0.8736007809638977\n",
      "Epoch 680: train loss: 0.8733174800872803\n",
      "Epoch 681: train loss: 0.8736515641212463\n",
      "Epoch 682: train loss: 0.8740053176879883\n",
      "Epoch 683: train loss: 0.8736506700515747\n",
      "Epoch 684: train loss: 0.8729039430618286\n",
      "Epoch 685: train loss: 0.8732392191886902\n",
      "Epoch 686: train loss: 0.8727397918701172\n",
      "Epoch 687: train loss: 0.8724663853645325\n",
      "Epoch 688: train loss: 0.8719443082809448\n",
      "Epoch 689: train loss: 0.8729562759399414\n",
      "Epoch 690: train loss: 0.872520387172699\n",
      "Epoch 691: train loss: 0.872839093208313\n",
      "Epoch 692: train loss: 0.8722397089004517\n",
      "Epoch 693: train loss: 0.872664749622345\n",
      "Epoch 694: train loss: 0.8728677034378052\n",
      "Epoch 695: train loss: 0.8714840412139893\n",
      "Epoch 696: train loss: 0.8715376853942871\n",
      "Epoch 697: train loss: 0.870564341545105\n",
      "Epoch 698: train loss: 0.8703299164772034\n",
      "Epoch 699: train loss: 0.8716124296188354\n",
      "Epoch 700: train loss: 0.8711819648742676\n",
      "Epoch 701: train loss: 0.8708842396736145\n",
      "Epoch 702: train loss: 0.8707930445671082\n",
      "Epoch 703: train loss: 0.871416449546814\n",
      "Epoch 704: train loss: 0.8703563809394836\n",
      "Epoch 705: train loss: 0.8700747489929199\n",
      "Epoch 706: train loss: 0.8701364398002625\n",
      "Epoch 707: train loss: 0.870570182800293\n",
      "Epoch 708: train loss: 0.8700337409973145\n",
      "Epoch 709: train loss: 0.8697369694709778\n",
      "Epoch 710: train loss: 0.8702141046524048\n",
      "Epoch 711: train loss: 0.8698587417602539\n",
      "Epoch 712: train loss: 0.8693326115608215\n",
      "Epoch 713: train loss: 0.8689064383506775\n",
      "Epoch 714: train loss: 0.8695487976074219\n",
      "Epoch 715: train loss: 0.8683948516845703\n",
      "Epoch 716: train loss: 0.8692063689231873\n",
      "Epoch 717: train loss: 0.8687722682952881\n",
      "Epoch 718: train loss: 0.8695921897888184\n",
      "Epoch 719: train loss: 0.8688289523124695\n",
      "Epoch 720: train loss: 0.8678318858146667\n",
      "Epoch 721: train loss: 0.8686633706092834\n",
      "Epoch 722: train loss: 0.8683215975761414\n",
      "Epoch 723: train loss: 0.8679024577140808\n",
      "Epoch 724: train loss: 0.8681960701942444\n",
      "Epoch 725: train loss: 0.8682001829147339\n",
      "Epoch 726: train loss: 0.8676350116729736\n",
      "Epoch 727: train loss: 0.86778324842453\n",
      "Epoch 728: train loss: 0.8676480054855347\n",
      "Epoch 729: train loss: 0.8675416111946106\n",
      "Epoch 730: train loss: 0.8677443861961365\n",
      "Epoch 731: train loss: 0.8671972751617432\n",
      "Epoch 732: train loss: 0.866415798664093\n",
      "Epoch 733: train loss: 0.8668394684791565\n",
      "Epoch 734: train loss: 0.8668299317359924\n",
      "Epoch 735: train loss: 0.8663573861122131\n",
      "Epoch 736: train loss: 0.8663568496704102\n",
      "Epoch 737: train loss: 0.8660751581192017\n",
      "Epoch 738: train loss: 0.8662305474281311\n",
      "Epoch 739: train loss: 0.8667938113212585\n",
      "Epoch 740: train loss: 0.866565465927124\n",
      "Epoch 741: train loss: 0.8656653761863708\n",
      "Epoch 742: train loss: 0.8659384250640869\n",
      "Epoch 743: train loss: 0.8659704923629761\n",
      "Epoch 744: train loss: 0.8661300539970398\n",
      "Epoch 745: train loss: 0.8647104501724243\n",
      "Epoch 746: train loss: 0.8645727038383484\n",
      "Epoch 747: train loss: 0.8650391101837158\n",
      "Epoch 748: train loss: 0.8649062514305115\n",
      "Epoch 749: train loss: 0.864065408706665\n",
      "Epoch 750: train loss: 0.864359974861145\n",
      "Epoch 751: train loss: 0.8639164566993713\n",
      "Epoch 752: train loss: 0.8632875084877014\n",
      "Epoch 753: train loss: 0.8641591668128967\n",
      "Epoch 754: train loss: 0.863441526889801\n",
      "Epoch 755: train loss: 0.8639204502105713\n",
      "Epoch 756: train loss: 0.8635649085044861\n",
      "Epoch 757: train loss: 0.8638055920600891\n",
      "Epoch 758: train loss: 0.8623990416526794\n",
      "Epoch 759: train loss: 0.8634302616119385\n",
      "Epoch 760: train loss: 0.8621829748153687\n",
      "Epoch 761: train loss: 0.8637181520462036\n",
      "Epoch 762: train loss: 0.8618721961975098\n",
      "Epoch 763: train loss: 0.8627755641937256\n",
      "Epoch 764: train loss: 0.8626922369003296\n",
      "Epoch 765: train loss: 0.8619117736816406\n",
      "Epoch 766: train loss: 0.8618097305297852\n",
      "Epoch 767: train loss: 0.8618142604827881\n",
      "Epoch 768: train loss: 0.8618867993354797\n",
      "Epoch 769: train loss: 0.8616044521331787\n",
      "Epoch 770: train loss: 0.8610895872116089\n",
      "Epoch 771: train loss: 0.8615930676460266\n",
      "Epoch 772: train loss: 0.8611992597579956\n",
      "Epoch 773: train loss: 0.8602116107940674\n",
      "Epoch 774: train loss: 0.8621681332588196\n",
      "Epoch 775: train loss: 0.8609991073608398\n",
      "Epoch 776: train loss: 0.86098712682724\n",
      "Epoch 777: train loss: 0.860173761844635\n",
      "Epoch 778: train loss: 0.8609675765037537\n",
      "Epoch 779: train loss: 0.8596201539039612\n",
      "Epoch 780: train loss: 0.8600587248802185\n",
      "Epoch 781: train loss: 0.8607103824615479\n",
      "Epoch 782: train loss: 0.860160768032074\n",
      "Epoch 783: train loss: 0.8594624996185303\n",
      "Epoch 784: train loss: 0.8600051403045654\n",
      "Epoch 785: train loss: 0.8600640296936035\n",
      "Epoch 786: train loss: 0.8589391708374023\n",
      "Epoch 787: train loss: 0.8584553599357605\n",
      "Epoch 788: train loss: 0.8593427538871765\n",
      "Epoch 789: train loss: 0.8589432239532471\n",
      "Epoch 790: train loss: 0.859947919845581\n",
      "Epoch 791: train loss: 0.859092116355896\n",
      "Epoch 792: train loss: 0.8582616448402405\n",
      "Epoch 793: train loss: 0.8584041595458984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 794: train loss: 0.8588976860046387\n",
      "Epoch 795: train loss: 0.8586576581001282\n",
      "Epoch 796: train loss: 0.8585212230682373\n",
      "Epoch 797: train loss: 0.8578365445137024\n",
      "Epoch 798: train loss: 0.8578678369522095\n",
      "Epoch 799: train loss: 0.8573806285858154\n",
      "Epoch 800: train loss: 0.8580658435821533\n",
      "Epoch 801: train loss: 0.8575142025947571\n",
      "Epoch 802: train loss: 0.8572975397109985\n",
      "Epoch 803: train loss: 0.8574376702308655\n",
      "Epoch 804: train loss: 0.8565758466720581\n",
      "Epoch 805: train loss: 0.8567678928375244\n",
      "Epoch 806: train loss: 0.8568585515022278\n",
      "Epoch 807: train loss: 0.8568171858787537\n",
      "Epoch 808: train loss: 0.8564252257347107\n",
      "Epoch 809: train loss: 0.855866014957428\n",
      "Epoch 810: train loss: 0.8564833402633667\n",
      "Epoch 811: train loss: 0.8566352128982544\n",
      "Epoch 812: train loss: 0.8555553555488586\n",
      "Epoch 813: train loss: 0.8556089401245117\n",
      "Epoch 814: train loss: 0.8554184436798096\n",
      "Epoch 815: train loss: 0.8559678196907043\n",
      "Epoch 816: train loss: 0.8547782301902771\n",
      "Epoch 817: train loss: 0.8554273247718811\n",
      "Epoch 818: train loss: 0.8542435765266418\n",
      "Epoch 819: train loss: 0.8551161885261536\n",
      "Epoch 820: train loss: 0.8544654846191406\n",
      "Epoch 821: train loss: 0.8549429178237915\n",
      "Epoch 822: train loss: 0.8547373414039612\n",
      "Epoch 823: train loss: 0.8546543121337891\n",
      "Epoch 824: train loss: 0.8534653186798096\n",
      "Epoch 825: train loss: 0.8533462285995483\n",
      "Epoch 826: train loss: 0.8538652062416077\n",
      "Epoch 827: train loss: 0.8538694381713867\n",
      "Epoch 828: train loss: 0.8535231947898865\n",
      "Epoch 829: train loss: 0.8536679148674011\n",
      "Epoch 830: train loss: 0.8538474440574646\n",
      "Epoch 831: train loss: 0.8528954386711121\n",
      "Epoch 832: train loss: 0.8524327874183655\n",
      "Epoch 833: train loss: 0.852840781211853\n",
      "Epoch 834: train loss: 0.8519635200500488\n",
      "Epoch 835: train loss: 0.8522000312805176\n",
      "Epoch 836: train loss: 0.8517780900001526\n",
      "Epoch 837: train loss: 0.8516741991043091\n",
      "Epoch 838: train loss: 0.8511741757392883\n",
      "Epoch 839: train loss: 0.8527079820632935\n",
      "Epoch 840: train loss: 0.8514955639839172\n",
      "Epoch 841: train loss: 0.8516560792922974\n",
      "Epoch 842: train loss: 0.8505669832229614\n",
      "Epoch 843: train loss: 0.8513144254684448\n",
      "Epoch 844: train loss: 0.8501772284507751\n",
      "Epoch 845: train loss: 0.8505088686943054\n",
      "Epoch 846: train loss: 0.8505033254623413\n",
      "Epoch 847: train loss: 0.8503352403640747\n",
      "Epoch 848: train loss: 0.8508345484733582\n",
      "Epoch 849: train loss: 0.849812924861908\n",
      "Epoch 850: train loss: 0.8499005436897278\n",
      "Epoch 851: train loss: 0.8500264883041382\n",
      "Epoch 852: train loss: 0.8502342104911804\n",
      "Epoch 853: train loss: 0.8491588830947876\n",
      "Epoch 854: train loss: 0.8491771221160889\n",
      "Epoch 855: train loss: 0.8483578562736511\n",
      "Epoch 856: train loss: 0.8494417667388916\n",
      "Epoch 857: train loss: 0.8488602638244629\n",
      "Epoch 858: train loss: 0.8489860892295837\n",
      "Epoch 859: train loss: 0.8485482335090637\n",
      "Epoch 860: train loss: 0.8473727107048035\n",
      "Epoch 861: train loss: 0.8479897975921631\n",
      "Epoch 862: train loss: 0.8479750752449036\n",
      "Epoch 863: train loss: 0.847748875617981\n",
      "Epoch 864: train loss: 0.8478504419326782\n",
      "Epoch 865: train loss: 0.8465273380279541\n",
      "Epoch 866: train loss: 0.8471852540969849\n",
      "Epoch 867: train loss: 0.8474788069725037\n",
      "Epoch 868: train loss: 0.8469964265823364\n",
      "Epoch 869: train loss: 0.8464149832725525\n",
      "Epoch 870: train loss: 0.8471473455429077\n",
      "Epoch 871: train loss: 0.847407877445221\n",
      "Epoch 872: train loss: 0.8465279340744019\n",
      "Epoch 873: train loss: 0.8454383015632629\n",
      "Epoch 874: train loss: 0.8458660840988159\n",
      "Epoch 875: train loss: 0.8459932804107666\n",
      "Epoch 876: train loss: 0.8463734984397888\n",
      "Epoch 877: train loss: 0.8459521532058716\n",
      "Epoch 878: train loss: 0.8452715277671814\n",
      "Epoch 879: train loss: 0.8452568650245667\n",
      "Epoch 880: train loss: 0.845492422580719\n",
      "Epoch 881: train loss: 0.8448829650878906\n",
      "Epoch 882: train loss: 0.844046950340271\n",
      "Epoch 883: train loss: 0.8446575999259949\n",
      "Epoch 884: train loss: 0.8447237014770508\n",
      "Epoch 885: train loss: 0.8438680171966553\n",
      "Epoch 886: train loss: 0.8440847992897034\n",
      "Epoch 887: train loss: 0.8443653583526611\n",
      "Epoch 888: train loss: 0.8433952331542969\n",
      "Epoch 889: train loss: 0.8436437845230103\n",
      "Epoch 890: train loss: 0.8436828255653381\n",
      "Epoch 891: train loss: 0.8426573872566223\n",
      "Epoch 892: train loss: 0.8429737687110901\n",
      "Epoch 893: train loss: 0.8422688245773315\n",
      "Epoch 894: train loss: 0.8425447344779968\n",
      "Epoch 895: train loss: 0.8421767950057983\n",
      "Epoch 896: train loss: 0.8415839672088623\n",
      "Epoch 897: train loss: 0.8419256210327148\n",
      "Epoch 898: train loss: 0.842534065246582\n",
      "Epoch 899: train loss: 0.8409873843193054\n",
      "Epoch 900: train loss: 0.8414925336837769\n",
      "Epoch 901: train loss: 0.8417797088623047\n",
      "Epoch 902: train loss: 0.8409959077835083\n",
      "Epoch 903: train loss: 0.8415887951850891\n",
      "Epoch 904: train loss: 0.8402575850486755\n",
      "Epoch 905: train loss: 0.8401458263397217\n",
      "Epoch 906: train loss: 0.8401594758033752\n",
      "Epoch 907: train loss: 0.8407049775123596\n",
      "Epoch 908: train loss: 0.8407756686210632\n",
      "Epoch 909: train loss: 0.8399606347084045\n",
      "Epoch 910: train loss: 0.8400954008102417\n",
      "Epoch 911: train loss: 0.839317798614502\n",
      "Epoch 912: train loss: 0.8399198651313782\n",
      "Epoch 913: train loss: 0.8389841318130493\n",
      "Epoch 914: train loss: 0.839408814907074\n",
      "Epoch 915: train loss: 0.8390622138977051\n",
      "Epoch 916: train loss: 0.8390085101127625\n",
      "Epoch 917: train loss: 0.8389169573783875\n",
      "Epoch 918: train loss: 0.8383898735046387\n",
      "Epoch 919: train loss: 0.8384388089179993\n",
      "Epoch 920: train loss: 0.838376522064209\n",
      "Epoch 921: train loss: 0.8385667204856873\n",
      "Epoch 922: train loss: 0.8378143310546875\n",
      "Epoch 923: train loss: 0.8380498290061951\n",
      "Epoch 924: train loss: 0.837405264377594\n",
      "Epoch 925: train loss: 0.8372732996940613\n",
      "Epoch 926: train loss: 0.837189257144928\n",
      "Epoch 927: train loss: 0.8374367952346802\n",
      "Epoch 928: train loss: 0.83687824010849\n",
      "Epoch 929: train loss: 0.8366039991378784\n",
      "Epoch 930: train loss: 0.8368116021156311\n",
      "Epoch 931: train loss: 0.8361911177635193\n",
      "Epoch 932: train loss: 0.8357079029083252\n",
      "Epoch 933: train loss: 0.8361454606056213\n",
      "Epoch 934: train loss: 0.8356266617774963\n",
      "Epoch 935: train loss: 0.8359540104866028\n",
      "Epoch 936: train loss: 0.8352288007736206\n",
      "Epoch 937: train loss: 0.835686981678009\n",
      "Epoch 938: train loss: 0.8350239396095276\n",
      "Epoch 939: train loss: 0.8349712491035461\n",
      "Epoch 940: train loss: 0.8350188136100769\n",
      "Epoch 941: train loss: 0.8343868255615234\n",
      "Epoch 942: train loss: 0.8343018293380737\n",
      "Epoch 943: train loss: 0.834297776222229\n",
      "Epoch 944: train loss: 0.834408700466156\n",
      "Epoch 945: train loss: 0.8343215584754944\n",
      "Epoch 946: train loss: 0.8342006206512451\n",
      "Epoch 947: train loss: 0.8336954712867737\n",
      "Epoch 948: train loss: 0.8333740830421448\n",
      "Epoch 949: train loss: 0.8338667750358582\n",
      "Epoch 950: train loss: 0.8328745365142822\n",
      "Epoch 951: train loss: 0.8328907489776611\n",
      "Epoch 952: train loss: 0.8329715728759766\n",
      "Epoch 953: train loss: 0.8326361179351807\n",
      "Epoch 954: train loss: 0.8324188590049744\n",
      "Epoch 955: train loss: 0.8325391411781311\n",
      "Epoch 956: train loss: 0.8327764868736267\n",
      "Epoch 957: train loss: 0.8319748044013977\n",
      "Epoch 958: train loss: 0.8318572640419006\n",
      "Epoch 959: train loss: 0.8313119411468506\n",
      "Epoch 960: train loss: 0.8315684199333191\n",
      "Epoch 961: train loss: 0.8319023251533508\n",
      "Epoch 962: train loss: 0.8316112160682678\n",
      "Epoch 963: train loss: 0.8317307233810425\n",
      "Epoch 964: train loss: 0.8312363624572754\n",
      "Epoch 965: train loss: 0.8311746716499329\n",
      "Epoch 966: train loss: 0.8309656977653503\n",
      "Epoch 967: train loss: 0.8302696943283081\n",
      "Epoch 968: train loss: 0.8303026556968689\n",
      "Epoch 969: train loss: 0.8302478790283203\n",
      "Epoch 970: train loss: 0.8302320837974548\n",
      "Epoch 971: train loss: 0.8302515745162964\n",
      "Epoch 972: train loss: 0.8294230699539185\n",
      "Epoch 973: train loss: 0.8302884697914124\n",
      "Epoch 974: train loss: 0.8286722302436829\n",
      "Epoch 975: train loss: 0.829899787902832\n",
      "Epoch 976: train loss: 0.8287904858589172\n",
      "Epoch 977: train loss: 0.8287971615791321\n",
      "Epoch 978: train loss: 0.8292589783668518\n",
      "Epoch 979: train loss: 0.8284361362457275\n",
      "Epoch 980: train loss: 0.8284268379211426\n",
      "Epoch 981: train loss: 0.8279322385787964\n",
      "Epoch 982: train loss: 0.8280436396598816\n",
      "Epoch 983: train loss: 0.8279460668563843\n",
      "Epoch 984: train loss: 0.8278608918190002\n",
      "Epoch 985: train loss: 0.8287267684936523\n",
      "Epoch 986: train loss: 0.827863872051239\n",
      "Epoch 987: train loss: 0.8270969986915588\n",
      "Epoch 988: train loss: 0.8272841572761536\n",
      "Epoch 989: train loss: 0.8270431160926819\n",
      "Epoch 990: train loss: 0.8267353773117065\n",
      "Epoch 991: train loss: 0.8260524272918701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 992: train loss: 0.8268043994903564\n",
      "Epoch 993: train loss: 0.8266156911849976\n",
      "Epoch 994: train loss: 0.8265127539634705\n",
      "Epoch 995: train loss: 0.8264734148979187\n",
      "Epoch 996: train loss: 0.8266292810440063\n",
      "Epoch 997: train loss: 0.8254895806312561\n",
      "Epoch 998: train loss: 0.8258985877037048\n",
      "Epoch 999: train loss: 0.8249511122703552\n",
      "Epoch 1000: train loss: 0.8245444893836975\n",
      "Epoch 1001: train loss: 0.8258901238441467\n",
      "Epoch 1002: train loss: 0.8253310918807983\n",
      "Epoch 1003: train loss: 0.8245411515235901\n",
      "Epoch 1004: train loss: 0.8244166970252991\n",
      "Epoch 1005: train loss: 0.8251355886459351\n",
      "Epoch 1006: train loss: 0.8244457840919495\n",
      "Epoch 1007: train loss: 0.8243008255958557\n",
      "Epoch 1008: train loss: 0.8240758776664734\n",
      "Epoch 1009: train loss: 0.8243696689605713\n",
      "Epoch 1010: train loss: 0.8248439431190491\n",
      "Epoch 1011: train loss: 0.8238552808761597\n",
      "Epoch 1012: train loss: 0.823798656463623\n",
      "Epoch 1013: train loss: 0.8232885599136353\n",
      "Epoch 1014: train loss: 0.8232614994049072\n",
      "Epoch 1015: train loss: 0.8231785893440247\n",
      "Epoch 1016: train loss: 0.8232080936431885\n",
      "Epoch 1017: train loss: 0.8233482837677002\n",
      "Epoch 1018: train loss: 0.8226392269134521\n",
      "Epoch 1019: train loss: 0.8230378031730652\n",
      "Epoch 1020: train loss: 0.8226763606071472\n",
      "Epoch 1021: train loss: 0.822985827922821\n",
      "Epoch 1022: train loss: 0.8217800259590149\n",
      "Epoch 1023: train loss: 0.8221822381019592\n",
      "Epoch 1024: train loss: 0.8228304386138916\n",
      "Epoch 1025: train loss: 0.8212385773658752\n",
      "Epoch 1026: train loss: 0.8214008808135986\n",
      "Epoch 1027: train loss: 0.8211236596107483\n",
      "Epoch 1028: train loss: 0.8217411041259766\n",
      "Epoch 1029: train loss: 0.8215314149856567\n",
      "Epoch 1030: train loss: 0.8213342428207397\n",
      "Epoch 1031: train loss: 0.8211963772773743\n",
      "Epoch 1032: train loss: 0.8209683299064636\n",
      "Epoch 1033: train loss: 0.8201303482055664\n",
      "Epoch 1034: train loss: 0.8207644820213318\n",
      "Epoch 1035: train loss: 0.8206779360771179\n",
      "Epoch 1036: train loss: 0.8196192979812622\n",
      "Epoch 1037: train loss: 0.8207186460494995\n",
      "Epoch 1038: train loss: 0.8194329738616943\n",
      "Epoch 1039: train loss: 0.819652259349823\n",
      "Epoch 1040: train loss: 0.8193604946136475\n",
      "Epoch 1041: train loss: 0.819783627986908\n",
      "Epoch 1042: train loss: 0.81973797082901\n",
      "Epoch 1043: train loss: 0.8203418254852295\n",
      "Epoch 1044: train loss: 0.818895697593689\n",
      "Epoch 1045: train loss: 0.8189101219177246\n",
      "Epoch 1046: train loss: 0.819153904914856\n",
      "Epoch 1047: train loss: 0.8184633255004883\n",
      "Epoch 1048: train loss: 0.8189631700515747\n",
      "Epoch 1049: train loss: 0.8184686899185181\n",
      "Epoch 1050: train loss: 0.817568838596344\n",
      "Epoch 1051: train loss: 0.8185445070266724\n",
      "Epoch 1052: train loss: 0.8182970881462097\n",
      "Epoch 1053: train loss: 0.8178439140319824\n",
      "Epoch 1054: train loss: 0.8182873129844666\n",
      "Epoch 1055: train loss: 0.8179049491882324\n",
      "Epoch 1056: train loss: 0.8178633451461792\n",
      "Epoch 1057: train loss: 0.8176753520965576\n",
      "Epoch 1058: train loss: 0.8172132968902588\n",
      "Epoch 1059: train loss: 0.8173418641090393\n",
      "Epoch 1060: train loss: 0.8169693350791931\n",
      "Epoch 1061: train loss: 0.8169177174568176\n",
      "Epoch 1062: train loss: 0.8172204494476318\n",
      "Epoch 1063: train loss: 0.8171225190162659\n",
      "Epoch 1064: train loss: 0.8168438673019409\n",
      "Epoch 1065: train loss: 0.8166803121566772\n",
      "Epoch 1066: train loss: 0.8165865540504456\n",
      "Epoch 1067: train loss: 0.8165873289108276\n",
      "Epoch 1068: train loss: 0.8165806531906128\n",
      "Epoch 1069: train loss: 0.8166926503181458\n",
      "Epoch 1070: train loss: 0.8170957565307617\n",
      "Epoch 1071: train loss: 0.8154589533805847\n",
      "Epoch 1072: train loss: 0.8155194520950317\n",
      "Epoch 1073: train loss: 0.8156611919403076\n",
      "Epoch 1074: train loss: 0.8152161240577698\n",
      "Epoch 1075: train loss: 0.815575897693634\n",
      "Epoch 1076: train loss: 0.8150708675384521\n",
      "Epoch 1077: train loss: 0.8154323101043701\n",
      "Epoch 1078: train loss: 0.8147850632667542\n",
      "Epoch 1079: train loss: 0.8143417835235596\n",
      "Epoch 1080: train loss: 0.8151311278343201\n",
      "Epoch 1081: train loss: 0.8144867420196533\n",
      "Epoch 1082: train loss: 0.8147023916244507\n",
      "Epoch 1083: train loss: 0.8149347901344299\n",
      "Epoch 1084: train loss: 0.814522385597229\n",
      "Epoch 1085: train loss: 0.8145027160644531\n",
      "Epoch 1086: train loss: 0.814303994178772\n",
      "Epoch 1087: train loss: 0.8137350678443909\n",
      "Epoch 1088: train loss: 0.8136364817619324\n",
      "Epoch 1089: train loss: 0.8128882646560669\n",
      "Epoch 1090: train loss: 0.8134987354278564\n",
      "Epoch 1091: train loss: 0.8126364350318909\n",
      "Epoch 1092: train loss: 0.8129499554634094\n",
      "Epoch 1093: train loss: 0.8135713338851929\n",
      "Epoch 1094: train loss: 0.8128149509429932\n",
      "Epoch 1095: train loss: 0.8128077387809753\n",
      "Epoch 1096: train loss: 0.8122947216033936\n",
      "Epoch 1097: train loss: 0.811579167842865\n",
      "Epoch 1098: train loss: 0.8122799396514893\n",
      "Epoch 1099: train loss: 0.8121981024742126\n",
      "Epoch 1100: train loss: 0.8120856881141663\n",
      "Epoch 1101: train loss: 0.8111403584480286\n",
      "Epoch 1102: train loss: 0.8120526671409607\n",
      "Epoch 1103: train loss: 0.8117434978485107\n",
      "Epoch 1104: train loss: 0.8118134140968323\n",
      "Epoch 1105: train loss: 0.8112599849700928\n",
      "Epoch 1106: train loss: 0.8114649653434753\n",
      "Epoch 1107: train loss: 0.8108006119728088\n",
      "Epoch 1108: train loss: 0.811343789100647\n",
      "Epoch 1109: train loss: 0.8104672431945801\n",
      "Epoch 1110: train loss: 0.8111097812652588\n",
      "Epoch 1111: train loss: 0.8105603456497192\n",
      "Epoch 1112: train loss: 0.8096534609794617\n",
      "Epoch 1113: train loss: 0.8095198273658752\n",
      "Epoch 1114: train loss: 0.8101075291633606\n",
      "Epoch 1115: train loss: 0.8096044063568115\n",
      "Epoch 1116: train loss: 0.8094169497489929\n",
      "Epoch 1117: train loss: 0.8096952438354492\n",
      "Epoch 1118: train loss: 0.8095684051513672\n",
      "Epoch 1119: train loss: 0.8092906475067139\n",
      "Epoch 1120: train loss: 0.8099501132965088\n",
      "Epoch 1121: train loss: 0.8093001246452332\n",
      "Epoch 1122: train loss: 0.8089340329170227\n",
      "Epoch 1123: train loss: 0.8088745474815369\n",
      "Epoch 1124: train loss: 0.8093404769897461\n",
      "Epoch 1125: train loss: 0.8080196976661682\n",
      "Epoch 1126: train loss: 0.8085905313491821\n",
      "Epoch 1127: train loss: 0.8080170750617981\n",
      "Epoch 1128: train loss: 0.8091222047805786\n",
      "Epoch 1129: train loss: 0.8084969520568848\n",
      "Epoch 1130: train loss: 0.8081504702568054\n",
      "Epoch 1131: train loss: 0.807434618473053\n",
      "Epoch 1132: train loss: 0.8080686330795288\n",
      "Epoch 1133: train loss: 0.8078879117965698\n",
      "Epoch 1134: train loss: 0.80777508020401\n",
      "Epoch 1135: train loss: 0.8072443604469299\n",
      "Epoch 1136: train loss: 0.8072420954704285\n",
      "Epoch 1137: train loss: 0.8072594404220581\n",
      "Epoch 1138: train loss: 0.8071945905685425\n",
      "Epoch 1139: train loss: 0.8067836165428162\n",
      "Epoch 1140: train loss: 0.8069300055503845\n",
      "Epoch 1141: train loss: 0.8070027828216553\n",
      "Epoch 1142: train loss: 0.8063607811927795\n",
      "Epoch 1143: train loss: 0.8062361478805542\n",
      "Epoch 1144: train loss: 0.8067854642868042\n",
      "Epoch 1145: train loss: 0.8055893778800964\n",
      "Epoch 1146: train loss: 0.8062795996665955\n",
      "Epoch 1147: train loss: 0.8062981367111206\n",
      "Epoch 1148: train loss: 0.8061535954475403\n",
      "Epoch 1149: train loss: 0.806365430355072\n",
      "Epoch 1150: train loss: 0.8053472638130188\n",
      "Epoch 1151: train loss: 0.8059970736503601\n",
      "Epoch 1152: train loss: 0.8064776062965393\n",
      "Epoch 1153: train loss: 0.8054866790771484\n",
      "Epoch 1154: train loss: 0.8050345182418823\n",
      "Epoch 1155: train loss: 0.8048713207244873\n",
      "Epoch 1156: train loss: 0.8054540753364563\n",
      "Epoch 1157: train loss: 0.8056365251541138\n",
      "Epoch 1158: train loss: 0.8053061366081238\n",
      "Epoch 1159: train loss: 0.8043649792671204\n",
      "Epoch 1160: train loss: 0.8052613139152527\n",
      "Epoch 1161: train loss: 0.8053472638130188\n",
      "Epoch 1162: train loss: 0.8047448396682739\n",
      "Epoch 1163: train loss: 0.8044333457946777\n",
      "Epoch 1164: train loss: 0.8039674162864685\n",
      "Epoch 1165: train loss: 0.8042318820953369\n",
      "Epoch 1166: train loss: 0.804129421710968\n",
      "Epoch 1167: train loss: 0.8043776154518127\n",
      "Epoch 1168: train loss: 0.8044673204421997\n",
      "Epoch 1169: train loss: 0.8034241199493408\n",
      "Epoch 1170: train loss: 0.8038517236709595\n",
      "Epoch 1171: train loss: 0.8030539155006409\n",
      "Epoch 1172: train loss: 0.8037537336349487\n",
      "Epoch 1173: train loss: 0.8033607006072998\n",
      "Epoch 1174: train loss: 0.802605390548706\n",
      "Epoch 1175: train loss: 0.8030655384063721\n",
      "Epoch 1176: train loss: 0.8026322722434998\n",
      "Epoch 1177: train loss: 0.8028391003608704\n",
      "Epoch 1178: train loss: 0.8025736808776855\n",
      "Epoch 1179: train loss: 0.802736759185791\n",
      "Epoch 1180: train loss: 0.8032264113426208\n",
      "Epoch 1181: train loss: 0.8017578125\n",
      "Epoch 1182: train loss: 0.8015351891517639\n",
      "Epoch 1183: train loss: 0.8021344542503357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1184: train loss: 0.8020749092102051\n",
      "Epoch 1185: train loss: 0.8012387156486511\n",
      "Epoch 1186: train loss: 0.8015164732933044\n",
      "Epoch 1187: train loss: 0.8012805581092834\n",
      "Epoch 1188: train loss: 0.8021226525306702\n",
      "Epoch 1189: train loss: 0.8014188408851624\n",
      "Epoch 1190: train loss: 0.8015398979187012\n",
      "Epoch 1191: train loss: 0.801098108291626\n",
      "Epoch 1192: train loss: 0.80096036195755\n",
      "Epoch 1193: train loss: 0.8009509444236755\n",
      "Epoch 1194: train loss: 0.8015203475952148\n",
      "Epoch 1195: train loss: 0.8008288145065308\n",
      "Epoch 1196: train loss: 0.8002798557281494\n",
      "Epoch 1197: train loss: 0.8006868362426758\n",
      "Epoch 1198: train loss: 0.7999071478843689\n",
      "Epoch 1199: train loss: 0.800258219242096\n",
      "Epoch 1200: train loss: 0.8002718091011047\n",
      "Epoch 1201: train loss: 0.8005651235580444\n",
      "Epoch 1202: train loss: 0.7996394634246826\n",
      "Epoch 1203: train loss: 0.8000035285949707\n",
      "Epoch 1204: train loss: 0.7996786832809448\n",
      "Epoch 1205: train loss: 0.7992231249809265\n",
      "Epoch 1206: train loss: 0.8004103899002075\n",
      "Epoch 1207: train loss: 0.8002063035964966\n",
      "Epoch 1208: train loss: 0.8001496195793152\n",
      "Epoch 1209: train loss: 0.7998448014259338\n",
      "Epoch 1210: train loss: 0.7988463044166565\n",
      "Epoch 1211: train loss: 0.7993568181991577\n",
      "Epoch 1212: train loss: 0.7987915873527527\n",
      "Epoch 1213: train loss: 0.7990373373031616\n",
      "Epoch 1214: train loss: 0.7994305491447449\n",
      "Epoch 1215: train loss: 0.7991087436676025\n",
      "Epoch 1216: train loss: 0.7977142333984375\n",
      "Epoch 1217: train loss: 0.7984059453010559\n",
      "Epoch 1218: train loss: 0.7991160154342651\n",
      "Epoch 1219: train loss: 0.798507809638977\n",
      "Epoch 1220: train loss: 0.7975041270256042\n",
      "Epoch 1221: train loss: 0.797573447227478\n",
      "Epoch 1222: train loss: 0.7984058856964111\n",
      "Epoch 1223: train loss: 0.7983354926109314\n",
      "Epoch 1224: train loss: 0.7977635860443115\n",
      "Epoch 1225: train loss: 0.7973849773406982\n",
      "Epoch 1226: train loss: 0.7975068092346191\n",
      "Epoch 1227: train loss: 0.7976590991020203\n",
      "Epoch 1228: train loss: 0.7975589632987976\n",
      "Epoch 1229: train loss: 0.7963747382164001\n",
      "Epoch 1230: train loss: 0.7970750331878662\n",
      "Epoch 1231: train loss: 0.7970697283744812\n",
      "Epoch 1232: train loss: 0.796481192111969\n",
      "Epoch 1233: train loss: 0.7975114583969116\n",
      "Epoch 1234: train loss: 0.7967607378959656\n",
      "Epoch 1235: train loss: 0.7968395948410034\n",
      "Epoch 1236: train loss: 0.7968001961708069\n",
      "Epoch 1237: train loss: 0.7967883944511414\n",
      "Epoch 1238: train loss: 0.7967264652252197\n",
      "Epoch 1239: train loss: 0.7960140109062195\n",
      "Epoch 1240: train loss: 0.7971095442771912\n",
      "Epoch 1241: train loss: 0.7964270710945129\n",
      "Epoch 1242: train loss: 0.7965087294578552\n",
      "Epoch 1243: train loss: 0.7963851690292358\n",
      "Epoch 1244: train loss: 0.7959032654762268\n",
      "Epoch 1245: train loss: 0.7960547804832458\n",
      "Epoch 1246: train loss: 0.7962797284126282\n",
      "Epoch 1247: train loss: 0.7962972521781921\n",
      "Epoch 1248: train loss: 0.7964125871658325\n",
      "Epoch 1249: train loss: 0.7953105568885803\n",
      "Epoch 1250: train loss: 0.7953705787658691\n",
      "Epoch 1251: train loss: 0.7957764267921448\n",
      "Epoch 1252: train loss: 0.7958064079284668\n",
      "Epoch 1253: train loss: 0.7951123714447021\n",
      "Epoch 1254: train loss: 0.7951153516769409\n",
      "Epoch 1255: train loss: 0.7956503033638\n",
      "Epoch 1256: train loss: 0.7959815263748169\n",
      "Epoch 1257: train loss: 0.7952972650527954\n",
      "Epoch 1258: train loss: 0.7946921586990356\n",
      "Epoch 1259: train loss: 0.7944161295890808\n",
      "Epoch 1260: train loss: 0.7949511408805847\n",
      "Epoch 1261: train loss: 0.794032096862793\n",
      "Epoch 1262: train loss: 0.7946839332580566\n",
      "Epoch 1263: train loss: 0.794369637966156\n",
      "Epoch 1264: train loss: 0.7952880859375\n",
      "Epoch 1265: train loss: 0.7941219210624695\n",
      "Epoch 1266: train loss: 0.7941229939460754\n",
      "Epoch 1267: train loss: 0.7938170433044434\n",
      "Epoch 1268: train loss: 0.7944040894508362\n",
      "Epoch 1269: train loss: 0.79380863904953\n",
      "Epoch 1270: train loss: 0.7936949133872986\n",
      "Epoch 1271: train loss: 0.7940377593040466\n",
      "Epoch 1272: train loss: 0.7934456467628479\n",
      "Epoch 1273: train loss: 0.793877124786377\n",
      "Epoch 1274: train loss: 0.7934619188308716\n",
      "Epoch 1275: train loss: 0.7932329177856445\n",
      "Epoch 1276: train loss: 0.7933671474456787\n",
      "Epoch 1277: train loss: 0.7929521203041077\n",
      "Epoch 1278: train loss: 0.7933332920074463\n",
      "Epoch 1279: train loss: 0.7928232550621033\n",
      "Epoch 1280: train loss: 0.7929496765136719\n",
      "Epoch 1281: train loss: 0.7930570840835571\n",
      "Epoch 1282: train loss: 0.7927583456039429\n",
      "Epoch 1283: train loss: 0.7926531434059143\n",
      "Epoch 1284: train loss: 0.7927021980285645\n",
      "Epoch 1285: train loss: 0.7924356460571289\n",
      "Epoch 1286: train loss: 0.7916250824928284\n",
      "Epoch 1287: train loss: 0.7925309538841248\n",
      "Epoch 1288: train loss: 0.7921211123466492\n",
      "Epoch 1289: train loss: 0.7920532822608948\n",
      "Epoch 1290: train loss: 0.7916401028633118\n",
      "Epoch 1291: train loss: 0.7917448878288269\n",
      "Epoch 1292: train loss: 0.7914252877235413\n",
      "Epoch 1293: train loss: 0.7917337417602539\n",
      "Epoch 1294: train loss: 0.7917152047157288\n",
      "Epoch 1295: train loss: 0.791401207447052\n",
      "Epoch 1296: train loss: 0.7914301156997681\n",
      "Epoch 1297: train loss: 0.7909202575683594\n",
      "Epoch 1298: train loss: 0.7910627126693726\n",
      "Epoch 1299: train loss: 0.7913170456886292\n",
      "Epoch 1300: train loss: 0.7911549806594849\n",
      "Epoch 1301: train loss: 0.7917247414588928\n",
      "Epoch 1302: train loss: 0.7906734347343445\n",
      "Epoch 1303: train loss: 0.7910996079444885\n",
      "Epoch 1304: train loss: 0.789960503578186\n",
      "Epoch 1305: train loss: 0.7896955609321594\n",
      "Epoch 1306: train loss: 0.7899520397186279\n",
      "Epoch 1307: train loss: 0.789707601070404\n",
      "Epoch 1308: train loss: 0.7897917628288269\n",
      "Epoch 1309: train loss: 0.7900465130805969\n",
      "Epoch 1310: train loss: 0.7898827791213989\n",
      "Epoch 1311: train loss: 0.7897710800170898\n",
      "Epoch 1312: train loss: 0.7891917824745178\n",
      "Epoch 1313: train loss: 0.7894921898841858\n",
      "Epoch 1314: train loss: 0.7894254922866821\n",
      "Epoch 1315: train loss: 0.7895440459251404\n",
      "Epoch 1316: train loss: 0.7889615297317505\n",
      "Epoch 1317: train loss: 0.7893100380897522\n",
      "Epoch 1318: train loss: 0.7894584536552429\n",
      "Epoch 1319: train loss: 0.7892019748687744\n",
      "Epoch 1320: train loss: 0.7888168692588806\n",
      "Epoch 1321: train loss: 0.7885605096817017\n",
      "Epoch 1322: train loss: 0.7893463969230652\n",
      "Epoch 1323: train loss: 0.7882634997367859\n",
      "Epoch 1324: train loss: 0.788925051689148\n",
      "Epoch 1325: train loss: 0.7884511947631836\n",
      "Epoch 1326: train loss: 0.7884917855262756\n",
      "Epoch 1327: train loss: 0.7882336974143982\n",
      "Epoch 1328: train loss: 0.7883792519569397\n",
      "Epoch 1329: train loss: 0.7883594036102295\n",
      "Epoch 1330: train loss: 0.7881020903587341\n",
      "Epoch 1331: train loss: 0.7877398729324341\n",
      "Epoch 1332: train loss: 0.7881141901016235\n",
      "Epoch 1333: train loss: 0.7884855270385742\n",
      "Epoch 1334: train loss: 0.7875831127166748\n",
      "Epoch 1335: train loss: 0.7875481247901917\n",
      "Epoch 1336: train loss: 0.7877020239830017\n",
      "Epoch 1337: train loss: 0.7873403429985046\n",
      "Epoch 1338: train loss: 0.7871263027191162\n",
      "Epoch 1339: train loss: 0.7873015999794006\n",
      "Epoch 1340: train loss: 0.7868712544441223\n",
      "Epoch 1341: train loss: 0.7872712016105652\n",
      "Epoch 1342: train loss: 0.7868394255638123\n",
      "Epoch 1343: train loss: 0.7871814966201782\n",
      "Epoch 1344: train loss: 0.7864910364151001\n",
      "Epoch 1345: train loss: 0.7862333655357361\n",
      "Epoch 1346: train loss: 0.7865720987319946\n",
      "Epoch 1347: train loss: 0.7859128713607788\n",
      "Epoch 1348: train loss: 0.786647379398346\n",
      "Epoch 1349: train loss: 0.7866173982620239\n",
      "Epoch 1350: train loss: 0.7852903604507446\n",
      "Epoch 1351: train loss: 0.7859601378440857\n",
      "Epoch 1352: train loss: 0.785514235496521\n",
      "Epoch 1353: train loss: 0.7859300374984741\n",
      "Epoch 1354: train loss: 0.7852542996406555\n",
      "Epoch 1355: train loss: 0.7863654494285583\n",
      "Epoch 1356: train loss: 0.7860691547393799\n",
      "Epoch 1357: train loss: 0.7849775552749634\n",
      "Epoch 1358: train loss: 0.785524845123291\n",
      "Epoch 1359: train loss: 0.785210371017456\n",
      "Epoch 1360: train loss: 0.7849543690681458\n",
      "Epoch 1361: train loss: 0.7855492234230042\n",
      "Epoch 1362: train loss: 0.7861925959587097\n",
      "Epoch 1363: train loss: 0.7858769297599792\n",
      "Epoch 1364: train loss: 0.7854035496711731\n",
      "Epoch 1365: train loss: 0.78456711769104\n",
      "Epoch 1366: train loss: 0.7856028079986572\n",
      "Epoch 1367: train loss: 0.7857826948165894\n",
      "Epoch 1368: train loss: 0.7846648693084717\n",
      "Epoch 1369: train loss: 0.7838430404663086\n",
      "Epoch 1370: train loss: 0.7846267819404602\n",
      "Epoch 1371: train loss: 0.784423291683197\n",
      "Epoch 1372: train loss: 0.7838788628578186\n",
      "Epoch 1373: train loss: 0.7842243909835815\n",
      "Epoch 1374: train loss: 0.7850163578987122\n",
      "Epoch 1375: train loss: 0.784065842628479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1376: train loss: 0.7842391133308411\n",
      "Epoch 1377: train loss: 0.7839873433113098\n",
      "Epoch 1378: train loss: 0.783164918422699\n",
      "Epoch 1379: train loss: 0.7835405468940735\n",
      "Epoch 1380: train loss: 0.783829927444458\n",
      "Epoch 1381: train loss: 0.7841142416000366\n",
      "Epoch 1382: train loss: 0.7831225991249084\n",
      "Epoch 1383: train loss: 0.7841551899909973\n",
      "Epoch 1384: train loss: 0.7838932871818542\n",
      "Epoch 1385: train loss: 0.7836940288543701\n",
      "Epoch 1386: train loss: 0.782917320728302\n",
      "Epoch 1387: train loss: 0.7831754088401794\n",
      "Epoch 1388: train loss: 0.7835087776184082\n",
      "Epoch 1389: train loss: 0.7834047079086304\n",
      "Epoch 1390: train loss: 0.7833272218704224\n",
      "Epoch 1391: train loss: 0.7839197516441345\n",
      "Epoch 1392: train loss: 0.7820904850959778\n",
      "Epoch 1393: train loss: 0.782770574092865\n",
      "Epoch 1394: train loss: 0.7824702858924866\n",
      "Epoch 1395: train loss: 0.7826172709465027\n",
      "Epoch 1396: train loss: 0.7827175259590149\n",
      "Epoch 1397: train loss: 0.782882809638977\n",
      "Epoch 1398: train loss: 0.7826213240623474\n",
      "Epoch 1399: train loss: 0.7825688123703003\n",
      "Epoch 1400: train loss: 0.7825126051902771\n",
      "Epoch 1401: train loss: 0.7817551493644714\n",
      "Epoch 1402: train loss: 0.7821527123451233\n",
      "Epoch 1403: train loss: 0.7822849750518799\n",
      "Epoch 1404: train loss: 0.7812934517860413\n",
      "Epoch 1405: train loss: 0.7814399003982544\n",
      "Epoch 1406: train loss: 0.7820635437965393\n",
      "Epoch 1407: train loss: 0.781221330165863\n",
      "Epoch 1408: train loss: 0.7811289429664612\n",
      "Epoch 1409: train loss: 0.7818858623504639\n",
      "Epoch 1410: train loss: 0.7811133861541748\n",
      "Epoch 1411: train loss: 0.7815383672714233\n",
      "Epoch 1412: train loss: 0.781238853931427\n",
      "Epoch 1413: train loss: 0.7816339135169983\n",
      "Epoch 1414: train loss: 0.7812608480453491\n",
      "Epoch 1415: train loss: 0.780951738357544\n",
      "Epoch 1416: train loss: 0.7811816930770874\n",
      "Epoch 1417: train loss: 0.7802891731262207\n",
      "Epoch 1418: train loss: 0.7804416418075562\n",
      "Epoch 1419: train loss: 0.7806853652000427\n",
      "Epoch 1420: train loss: 0.7807472348213196\n",
      "Epoch 1421: train loss: 0.7805947065353394\n",
      "Epoch 1422: train loss: 0.7801994681358337\n",
      "Epoch 1423: train loss: 0.77972811460495\n",
      "Epoch 1424: train loss: 0.7803146839141846\n",
      "Epoch 1425: train loss: 0.7805748581886292\n",
      "Epoch 1426: train loss: 0.7802565693855286\n",
      "Epoch 1427: train loss: 0.7796523571014404\n",
      "Epoch 1428: train loss: 0.7802982330322266\n",
      "Epoch 1429: train loss: 0.7797271013259888\n",
      "Epoch 1430: train loss: 0.7806261777877808\n",
      "Epoch 1431: train loss: 0.7799292206764221\n",
      "Epoch 1432: train loss: 0.7799878716468811\n",
      "Epoch 1433: train loss: 0.7801483273506165\n",
      "Epoch 1434: train loss: 0.7801958322525024\n",
      "Epoch 1435: train loss: 0.7796223759651184\n",
      "Epoch 1436: train loss: 0.7790551781654358\n",
      "Epoch 1437: train loss: 0.7799425721168518\n",
      "Epoch 1438: train loss: 0.7794208526611328\n",
      "Epoch 1439: train loss: 0.779493510723114\n",
      "Epoch 1440: train loss: 0.7791951298713684\n",
      "Epoch 1441: train loss: 0.7789631485939026\n",
      "Epoch 1442: train loss: 0.7781455516815186\n",
      "Epoch 1443: train loss: 0.7788578867912292\n",
      "Epoch 1444: train loss: 0.7787787318229675\n",
      "Epoch 1445: train loss: 0.7782089710235596\n",
      "Epoch 1446: train loss: 0.7793025970458984\n",
      "Epoch 1447: train loss: 0.778422474861145\n",
      "Epoch 1448: train loss: 0.7787606120109558\n",
      "Epoch 1449: train loss: 0.7783157825469971\n",
      "Epoch 1450: train loss: 0.7786494493484497\n",
      "Epoch 1451: train loss: 0.7781365513801575\n",
      "Epoch 1452: train loss: 0.7789818048477173\n",
      "Epoch 1453: train loss: 0.7787877321243286\n",
      "Epoch 1454: train loss: 0.7781491279602051\n",
      "Epoch 1455: train loss: 0.7783796787261963\n",
      "Epoch 1456: train loss: 0.7777306437492371\n",
      "Epoch 1457: train loss: 0.7781476378440857\n",
      "Epoch 1458: train loss: 0.7785466909408569\n",
      "Epoch 1459: train loss: 0.7780041694641113\n",
      "Epoch 1460: train loss: 0.7774236798286438\n",
      "Epoch 1461: train loss: 0.778594970703125\n",
      "Epoch 1462: train loss: 0.7776176929473877\n",
      "Epoch 1463: train loss: 0.7776408791542053\n",
      "Epoch 1464: train loss: 0.7778779864311218\n",
      "Epoch 1465: train loss: 0.7765552997589111\n",
      "Epoch 1466: train loss: 0.7774758338928223\n",
      "Epoch 1467: train loss: 0.7768220901489258\n",
      "Epoch 1468: train loss: 0.7774220108985901\n",
      "Epoch 1469: train loss: 0.7775253653526306\n",
      "Epoch 1470: train loss: 0.7771472334861755\n",
      "Epoch 1471: train loss: 0.777654767036438\n",
      "Epoch 1472: train loss: 0.7771643400192261\n",
      "Epoch 1473: train loss: 0.7767485976219177\n",
      "Epoch 1474: train loss: 0.7768591046333313\n",
      "Epoch 1475: train loss: 0.7767988443374634\n",
      "Epoch 1476: train loss: 0.7766547799110413\n",
      "Epoch 1477: train loss: 0.7761675715446472\n",
      "Epoch 1478: train loss: 0.776613175868988\n",
      "Epoch 1479: train loss: 0.7771320939064026\n",
      "Epoch 1480: train loss: 0.7770287394523621\n",
      "Epoch 1481: train loss: 0.7768614888191223\n",
      "Epoch 1482: train loss: 0.7767156958580017\n",
      "Epoch 1483: train loss: 0.7769307494163513\n",
      "Epoch 1484: train loss: 0.7777354121208191\n",
      "Epoch 1485: train loss: 0.7770501971244812\n",
      "Epoch 1486: train loss: 0.7762979865074158\n",
      "Epoch 1487: train loss: 0.7765222191810608\n",
      "Epoch 1488: train loss: 0.7757629752159119\n",
      "Epoch 1489: train loss: 0.7764139771461487\n",
      "Epoch 1490: train loss: 0.7756007313728333\n",
      "Epoch 1491: train loss: 0.7753678560256958\n",
      "Epoch 1492: train loss: 0.776005744934082\n",
      "Epoch 1493: train loss: 0.7761099934577942\n",
      "Epoch 1494: train loss: 0.7756791114807129\n",
      "Epoch 1495: train loss: 0.7751848101615906\n",
      "Epoch 1496: train loss: 0.7750803828239441\n",
      "Epoch 1497: train loss: 0.7752787470817566\n",
      "Epoch 1498: train loss: 0.7751435041427612\n",
      "Epoch 1499: train loss: 0.7750118374824524\n",
      "Epoch 1500: train loss: 0.7745071649551392\n",
      "Epoch 1501: train loss: 0.7755042314529419\n",
      "Epoch 1502: train loss: 0.7747043371200562\n",
      "Epoch 1503: train loss: 0.7747253179550171\n",
      "Epoch 1504: train loss: 0.7748716473579407\n",
      "Epoch 1505: train loss: 0.7752581238746643\n",
      "Epoch 1506: train loss: 0.7750161290168762\n",
      "Epoch 1507: train loss: 0.7745692729949951\n",
      "Epoch 1508: train loss: 0.7746656537055969\n",
      "Epoch 1509: train loss: 0.7746620774269104\n",
      "Epoch 1510: train loss: 0.7743395566940308\n",
      "Epoch 1511: train loss: 0.7751638889312744\n",
      "Epoch 1512: train loss: 0.7745755910873413\n",
      "Epoch 1513: train loss: 0.7742894887924194\n",
      "Epoch 1514: train loss: 0.7740504741668701\n",
      "Epoch 1515: train loss: 0.7743130922317505\n",
      "Epoch 1516: train loss: 0.7740307450294495\n",
      "Epoch 1517: train loss: 0.7742783427238464\n",
      "Epoch 1518: train loss: 0.7736163139343262\n",
      "Epoch 1519: train loss: 0.77419114112854\n",
      "Epoch 1520: train loss: 0.7740606665611267\n",
      "Epoch 1521: train loss: 0.7741789817810059\n",
      "Epoch 1522: train loss: 0.7734735608100891\n",
      "Epoch 1523: train loss: 0.7736048102378845\n",
      "Epoch 1524: train loss: 0.7738420367240906\n",
      "Epoch 1525: train loss: 0.7734469771385193\n",
      "Epoch 1526: train loss: 0.7733944058418274\n",
      "Epoch 1527: train loss: 0.7735456824302673\n",
      "Epoch 1528: train loss: 0.7730869054794312\n",
      "Epoch 1529: train loss: 0.7736117839813232\n",
      "Epoch 1530: train loss: 0.7732102870941162\n",
      "Epoch 1531: train loss: 0.773514449596405\n",
      "Epoch 1532: train loss: 0.7733560800552368\n",
      "Epoch 1533: train loss: 0.7736852765083313\n",
      "Epoch 1534: train loss: 0.7726913094520569\n",
      "Epoch 1535: train loss: 0.7726722359657288\n",
      "Epoch 1536: train loss: 0.7739447951316833\n",
      "Epoch 1537: train loss: 0.7729417681694031\n",
      "Epoch 1538: train loss: 0.7735341191291809\n",
      "Epoch 1539: train loss: 0.7731451988220215\n",
      "Epoch 1540: train loss: 0.7729828953742981\n",
      "Epoch 1541: train loss: 0.7721447944641113\n",
      "Epoch 1542: train loss: 0.7716297507286072\n",
      "Epoch 1543: train loss: 0.7726711630821228\n",
      "Epoch 1544: train loss: 0.7725255489349365\n",
      "Epoch 1545: train loss: 0.7728591561317444\n",
      "Epoch 1546: train loss: 0.7726322412490845\n",
      "Epoch 1547: train loss: 0.77214515209198\n",
      "Epoch 1548: train loss: 0.7716865539550781\n",
      "Epoch 1549: train loss: 0.7726932764053345\n",
      "Epoch 1550: train loss: 0.7723719477653503\n",
      "Epoch 1551: train loss: 0.7721043229103088\n",
      "Epoch 1552: train loss: 0.7715975642204285\n",
      "Epoch 1553: train loss: 0.7720556855201721\n",
      "Epoch 1554: train loss: 0.7720537781715393\n",
      "Epoch 1555: train loss: 0.7716697454452515\n",
      "Epoch 1556: train loss: 0.7720487117767334\n",
      "Epoch 1557: train loss: 0.7717070579528809\n",
      "Epoch 1558: train loss: 0.7715471386909485\n",
      "Epoch 1559: train loss: 0.7709775567054749\n",
      "Epoch 1560: train loss: 0.7716249227523804\n",
      "Epoch 1561: train loss: 0.7713109254837036\n",
      "Epoch 1562: train loss: 0.7703491449356079\n",
      "Epoch 1563: train loss: 0.7704556584358215\n",
      "Epoch 1564: train loss: 0.7710564732551575\n",
      "Epoch 1565: train loss: 0.7713179588317871\n",
      "Epoch 1566: train loss: 0.7709325551986694\n",
      "Epoch 1567: train loss: 0.7708361148834229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1568: train loss: 0.770662784576416\n",
      "Epoch 1569: train loss: 0.7709463238716125\n",
      "Epoch 1570: train loss: 0.7713378667831421\n",
      "Epoch 1571: train loss: 0.7708612084388733\n",
      "Epoch 1572: train loss: 0.7703925371170044\n",
      "Epoch 1573: train loss: 0.7706425786018372\n",
      "Epoch 1574: train loss: 0.7702545523643494\n",
      "Epoch 1575: train loss: 0.7701841592788696\n",
      "Epoch 1576: train loss: 0.7701174020767212\n",
      "Epoch 1577: train loss: 0.7692912817001343\n",
      "Epoch 1578: train loss: 0.7705177068710327\n",
      "Epoch 1579: train loss: 0.7705004215240479\n",
      "Epoch 1580: train loss: 0.7705134749412537\n",
      "Epoch 1581: train loss: 0.7698944211006165\n",
      "Epoch 1582: train loss: 0.7699933052062988\n",
      "Epoch 1583: train loss: 0.7701510787010193\n",
      "Epoch 1584: train loss: 0.7702596187591553\n",
      "Epoch 1585: train loss: 0.7699832916259766\n",
      "Epoch 1586: train loss: 0.7692247033119202\n",
      "Epoch 1587: train loss: 0.7692475318908691\n",
      "Epoch 1588: train loss: 0.7701194286346436\n",
      "Epoch 1589: train loss: 0.769612193107605\n",
      "Epoch 1590: train loss: 0.7689933776855469\n",
      "Epoch 1591: train loss: 0.7694088220596313\n",
      "Epoch 1592: train loss: 0.7689992785453796\n",
      "Epoch 1593: train loss: 0.7690731883049011\n",
      "Epoch 1594: train loss: 0.7696165442466736\n",
      "Epoch 1595: train loss: 0.7694971561431885\n",
      "Epoch 1596: train loss: 0.7693831324577332\n",
      "Epoch 1597: train loss: 0.7693648338317871\n",
      "Epoch 1598: train loss: 0.7695580720901489\n",
      "Epoch 1599: train loss: 0.7694994807243347\n",
      "Epoch 1600: train loss: 0.768839955329895\n",
      "Epoch 1601: train loss: 0.7683976292610168\n",
      "Epoch 1602: train loss: 0.7684478759765625\n",
      "Epoch 1603: train loss: 0.7692098021507263\n",
      "Epoch 1604: train loss: 0.7686720490455627\n",
      "Epoch 1605: train loss: 0.7684363126754761\n",
      "Epoch 1606: train loss: 0.7684652209281921\n",
      "Epoch 1607: train loss: 0.7688025236129761\n",
      "Epoch 1608: train loss: 0.7692745327949524\n",
      "Epoch 1609: train loss: 0.7676389813423157\n",
      "Epoch 1610: train loss: 0.7683948874473572\n",
      "Epoch 1611: train loss: 0.7685871124267578\n",
      "Epoch 1612: train loss: 0.7681666612625122\n",
      "Epoch 1613: train loss: 0.7684292793273926\n",
      "Epoch 1614: train loss: 0.7684947848320007\n",
      "Epoch 1615: train loss: 0.7692012786865234\n",
      "Epoch 1616: train loss: 0.7679079174995422\n",
      "Epoch 1617: train loss: 0.7687225937843323\n",
      "Epoch 1618: train loss: 0.7679138779640198\n",
      "Epoch 1619: train loss: 0.7675713896751404\n",
      "Epoch 1620: train loss: 0.768397867679596\n",
      "Epoch 1621: train loss: 0.7686974406242371\n",
      "Epoch 1622: train loss: 0.767012357711792\n",
      "Epoch 1623: train loss: 0.7671971321105957\n",
      "Epoch 1624: train loss: 0.7676308751106262\n",
      "Epoch 1625: train loss: 0.7681907415390015\n",
      "Epoch 1626: train loss: 0.7679036259651184\n",
      "Epoch 1627: train loss: 0.7679131627082825\n",
      "Epoch 1628: train loss: 0.7676268815994263\n",
      "Epoch 1629: train loss: 0.7674618363380432\n",
      "Epoch 1630: train loss: 0.7669909000396729\n",
      "Epoch 1631: train loss: 0.7673198580741882\n",
      "Epoch 1632: train loss: 0.7673274874687195\n",
      "Epoch 1633: train loss: 0.7675888538360596\n",
      "Epoch 1634: train loss: 0.7667171359062195\n",
      "Epoch 1635: train loss: 0.7668095231056213\n",
      "Epoch 1636: train loss: 0.76683109998703\n",
      "Epoch 1637: train loss: 0.7670758366584778\n",
      "Epoch 1638: train loss: 0.7674601674079895\n",
      "Epoch 1639: train loss: 0.7664572596549988\n",
      "Epoch 1640: train loss: 0.7667930126190186\n",
      "Epoch 1641: train loss: 0.7667647004127502\n",
      "Epoch 1642: train loss: 0.7664698958396912\n",
      "Epoch 1643: train loss: 0.7670294046401978\n",
      "Epoch 1644: train loss: 0.7660579085350037\n",
      "Epoch 1645: train loss: 0.7663483619689941\n",
      "Epoch 1646: train loss: 0.7666293978691101\n",
      "Epoch 1647: train loss: 0.7662011981010437\n",
      "Epoch 1648: train loss: 0.7661200165748596\n",
      "Epoch 1649: train loss: 0.7659626603126526\n",
      "Epoch 1650: train loss: 0.766973614692688\n",
      "Epoch 1651: train loss: 0.7663166522979736\n",
      "Epoch 1652: train loss: 0.7662630677223206\n",
      "Epoch 1653: train loss: 0.7659659385681152\n",
      "Epoch 1654: train loss: 0.7658865451812744\n",
      "Epoch 1655: train loss: 0.7660454511642456\n",
      "Epoch 1656: train loss: 0.7659412026405334\n",
      "Epoch 1657: train loss: 0.7660579085350037\n",
      "Epoch 1658: train loss: 0.7655635476112366\n",
      "Epoch 1659: train loss: 0.7664749026298523\n",
      "Epoch 1660: train loss: 0.7661411762237549\n",
      "Epoch 1661: train loss: 0.7654361128807068\n",
      "Epoch 1662: train loss: 0.7652627229690552\n",
      "Epoch 1663: train loss: 0.765007734298706\n",
      "Epoch 1664: train loss: 0.7651463747024536\n",
      "Epoch 1665: train loss: 0.7662621736526489\n",
      "Epoch 1666: train loss: 0.7652542591094971\n",
      "Epoch 1667: train loss: 0.7651602029800415\n",
      "Epoch 1668: train loss: 0.7646123766899109\n",
      "Epoch 1669: train loss: 0.7654112577438354\n",
      "Epoch 1670: train loss: 0.765320360660553\n",
      "Epoch 1671: train loss: 0.7658275365829468\n",
      "Epoch 1672: train loss: 0.7653800249099731\n",
      "Epoch 1673: train loss: 0.7656650543212891\n",
      "Epoch 1674: train loss: 0.7650297284126282\n",
      "Epoch 1675: train loss: 0.7651752829551697\n",
      "Epoch 1676: train loss: 0.7643845677375793\n",
      "Epoch 1677: train loss: 0.7643111348152161\n",
      "Epoch 1678: train loss: 0.7649824619293213\n",
      "Epoch 1679: train loss: 0.7647575736045837\n",
      "Epoch 1680: train loss: 0.7650896310806274\n",
      "Epoch 1681: train loss: 0.7635844945907593\n",
      "Epoch 1682: train loss: 0.7642725706100464\n",
      "Epoch 1683: train loss: 0.7643577456474304\n",
      "Epoch 1684: train loss: 0.7640312910079956\n",
      "Epoch 1685: train loss: 0.7639154195785522\n",
      "Epoch 1686: train loss: 0.7640754580497742\n",
      "Epoch 1687: train loss: 0.7645044326782227\n",
      "Epoch 1688: train loss: 0.7642341256141663\n",
      "Epoch 1689: train loss: 0.7642896771430969\n",
      "Epoch 1690: train loss: 0.7642477750778198\n",
      "Epoch 1691: train loss: 0.7636458277702332\n",
      "Epoch 1692: train loss: 0.7637910842895508\n",
      "Epoch 1693: train loss: 0.7638900876045227\n",
      "Epoch 1694: train loss: 0.7640901803970337\n",
      "Epoch 1695: train loss: 0.7635248899459839\n",
      "Epoch 1696: train loss: 0.7636433839797974\n",
      "Epoch 1697: train loss: 0.76352858543396\n",
      "Epoch 1698: train loss: 0.7633498311042786\n",
      "Epoch 1699: train loss: 0.7635877728462219\n",
      "Epoch 1700: train loss: 0.7632462978363037\n",
      "Epoch 1701: train loss: 0.7640437483787537\n",
      "Epoch 1702: train loss: 0.7635200023651123\n",
      "Epoch 1703: train loss: 0.7635684013366699\n",
      "Epoch 1704: train loss: 0.763447642326355\n",
      "Epoch 1705: train loss: 0.7629777789115906\n",
      "Epoch 1706: train loss: 0.763741672039032\n",
      "Epoch 1707: train loss: 0.7630490064620972\n",
      "Epoch 1708: train loss: 0.762321949005127\n",
      "Epoch 1709: train loss: 0.76285320520401\n",
      "Epoch 1710: train loss: 0.7630962133407593\n",
      "Epoch 1711: train loss: 0.7627123594284058\n",
      "Epoch 1712: train loss: 0.762470006942749\n",
      "Epoch 1713: train loss: 0.7629136443138123\n",
      "Epoch 1714: train loss: 0.7621922492980957\n",
      "Epoch 1715: train loss: 0.762744665145874\n",
      "Epoch 1716: train loss: 0.7623574733734131\n",
      "Epoch 1717: train loss: 0.7631289958953857\n",
      "Epoch 1718: train loss: 0.7625775933265686\n",
      "Epoch 1719: train loss: 0.7628577947616577\n",
      "Epoch 1720: train loss: 0.7621476650238037\n",
      "Epoch 1721: train loss: 0.7623568177223206\n",
      "Epoch 1722: train loss: 0.7621670365333557\n",
      "Epoch 1723: train loss: 0.7632128596305847\n",
      "Epoch 1724: train loss: 0.7623765468597412\n",
      "Epoch 1725: train loss: 0.762747049331665\n",
      "Epoch 1726: train loss: 0.7624312043190002\n",
      "Epoch 1727: train loss: 0.7623127698898315\n",
      "Epoch 1728: train loss: 0.761673092842102\n",
      "Epoch 1729: train loss: 0.7621651887893677\n",
      "Epoch 1730: train loss: 0.7615450024604797\n",
      "Epoch 1731: train loss: 0.7620831727981567\n",
      "Epoch 1732: train loss: 0.7624313235282898\n",
      "Epoch 1733: train loss: 0.7613311409950256\n",
      "Epoch 1734: train loss: 0.7615376114845276\n",
      "Epoch 1735: train loss: 0.7621912360191345\n",
      "Epoch 1736: train loss: 0.7618089914321899\n",
      "Epoch 1737: train loss: 0.7614555954933167\n",
      "Epoch 1738: train loss: 0.7616620063781738\n",
      "Epoch 1739: train loss: 0.7615867257118225\n",
      "Epoch 1740: train loss: 0.7616251111030579\n",
      "Epoch 1741: train loss: 0.7612178921699524\n",
      "Epoch 1742: train loss: 0.7614133358001709\n",
      "Epoch 1743: train loss: 0.7614569067955017\n",
      "Epoch 1744: train loss: 0.7616320252418518\n",
      "Epoch 1745: train loss: 0.7611373066902161\n",
      "Epoch 1746: train loss: 0.7612589597702026\n",
      "Epoch 1747: train loss: 0.7601497173309326\n",
      "Epoch 1748: train loss: 0.7608092427253723\n",
      "Epoch 1749: train loss: 0.7610263228416443\n",
      "Epoch 1750: train loss: 0.7612953782081604\n",
      "Epoch 1751: train loss: 0.7609791159629822\n",
      "Epoch 1752: train loss: 0.7611061930656433\n",
      "Epoch 1753: train loss: 0.7606496810913086\n",
      "Epoch 1754: train loss: 0.7607453465461731\n",
      "Epoch 1755: train loss: 0.7611932754516602\n",
      "Epoch 1756: train loss: 0.7609277963638306\n",
      "Epoch 1757: train loss: 0.7604715824127197\n",
      "Epoch 1758: train loss: 0.7605602145195007\n",
      "Epoch 1759: train loss: 0.7608089447021484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1760: train loss: 0.7603727579116821\n",
      "Epoch 1761: train loss: 0.760226309299469\n",
      "Epoch 1762: train loss: 0.760787844657898\n",
      "Epoch 1763: train loss: 0.7600641250610352\n",
      "Epoch 1764: train loss: 0.760696291923523\n",
      "Epoch 1765: train loss: 0.7602638006210327\n",
      "Epoch 1766: train loss: 0.760431706905365\n",
      "Epoch 1767: train loss: 0.7601416707038879\n",
      "Epoch 1768: train loss: 0.7615787386894226\n",
      "Epoch 1769: train loss: 0.7594478726387024\n",
      "Epoch 1770: train loss: 0.7602465748786926\n",
      "Epoch 1771: train loss: 0.7601361870765686\n",
      "Epoch 1772: train loss: 0.7603459358215332\n",
      "Epoch 1773: train loss: 0.7596668004989624\n",
      "Epoch 1774: train loss: 0.7598447203636169\n",
      "Epoch 1775: train loss: 0.7599672079086304\n",
      "Epoch 1776: train loss: 0.7596375942230225\n",
      "Epoch 1777: train loss: 0.7593364715576172\n",
      "Epoch 1778: train loss: 0.7599220871925354\n",
      "Epoch 1779: train loss: 0.7593362927436829\n",
      "Epoch 1780: train loss: 0.7598981857299805\n",
      "Epoch 1781: train loss: 0.7591469287872314\n",
      "Epoch 1782: train loss: 0.7589566707611084\n",
      "Epoch 1783: train loss: 0.7592813372612\n",
      "Epoch 1784: train loss: 0.7589820027351379\n",
      "Epoch 1785: train loss: 0.759527862071991\n",
      "Epoch 1786: train loss: 0.7595133185386658\n",
      "Epoch 1787: train loss: 0.758866012096405\n",
      "Epoch 1788: train loss: 0.7590913772583008\n",
      "Epoch 1789: train loss: 0.7586503624916077\n",
      "Epoch 1790: train loss: 0.7583149075508118\n",
      "Epoch 1791: train loss: 0.7590111494064331\n",
      "Epoch 1792: train loss: 0.758703351020813\n",
      "Epoch 1793: train loss: 0.759149968624115\n",
      "Epoch 1794: train loss: 0.7588254809379578\n",
      "Epoch 1795: train loss: 0.7587877511978149\n",
      "Epoch 1796: train loss: 0.7588657140731812\n",
      "Epoch 1797: train loss: 0.7592542767524719\n",
      "Epoch 1798: train loss: 0.7581175565719604\n",
      "Epoch 1799: train loss: 0.7581361532211304\n",
      "Epoch 1800: train loss: 0.757932722568512\n",
      "Epoch 1801: train loss: 0.757986307144165\n",
      "Epoch 1802: train loss: 0.7580199241638184\n",
      "Epoch 1803: train loss: 0.758590042591095\n",
      "Epoch 1804: train loss: 0.7577188014984131\n",
      "Epoch 1805: train loss: 0.7582228183746338\n",
      "Epoch 1806: train loss: 0.7580891847610474\n",
      "Epoch 1807: train loss: 0.7583327889442444\n",
      "Epoch 1808: train loss: 0.7584487199783325\n",
      "Epoch 1809: train loss: 0.7580192685127258\n",
      "Epoch 1810: train loss: 0.7583117485046387\n",
      "Epoch 1811: train loss: 0.757766842842102\n",
      "Epoch 1812: train loss: 0.7573375105857849\n",
      "Epoch 1813: train loss: 0.7585906982421875\n",
      "Epoch 1814: train loss: 0.7579967379570007\n",
      "Epoch 1815: train loss: 0.7578760385513306\n",
      "Epoch 1816: train loss: 0.7578980922698975\n",
      "Epoch 1817: train loss: 0.75738126039505\n",
      "Epoch 1818: train loss: 0.757683277130127\n",
      "Epoch 1819: train loss: 0.7583000063896179\n",
      "Epoch 1820: train loss: 0.7575485706329346\n",
      "Epoch 1821: train loss: 0.757419228553772\n",
      "Epoch 1822: train loss: 0.756992518901825\n",
      "Epoch 1823: train loss: 0.757158100605011\n",
      "Epoch 1824: train loss: 0.7575855851173401\n",
      "Epoch 1825: train loss: 0.7569676637649536\n",
      "Epoch 1826: train loss: 0.7580480575561523\n",
      "Epoch 1827: train loss: 0.7569006681442261\n",
      "Epoch 1828: train loss: 0.7564279437065125\n",
      "Epoch 1829: train loss: 0.7571824789047241\n",
      "Epoch 1830: train loss: 0.7574666142463684\n",
      "Epoch 1831: train loss: 0.7562642693519592\n",
      "Epoch 1832: train loss: 0.7563011050224304\n",
      "Epoch 1833: train loss: 0.7570000290870667\n",
      "Epoch 1834: train loss: 0.7563508152961731\n",
      "Epoch 1835: train loss: 0.7559525966644287\n",
      "Epoch 1836: train loss: 0.7569359540939331\n",
      "Epoch 1837: train loss: 0.7562139630317688\n",
      "Epoch 1838: train loss: 0.7565685510635376\n",
      "Epoch 1839: train loss: 0.7561330199241638\n",
      "Epoch 1840: train loss: 0.7561005353927612\n",
      "Epoch 1841: train loss: 0.7563539147377014\n",
      "Epoch 1842: train loss: 0.7561871409416199\n",
      "Epoch 1843: train loss: 0.7558596730232239\n",
      "Epoch 1844: train loss: 0.7552470564842224\n",
      "Epoch 1845: train loss: 0.7554119229316711\n",
      "Epoch 1846: train loss: 0.7560686469078064\n",
      "Epoch 1847: train loss: 0.7559266686439514\n",
      "Epoch 1848: train loss: 0.7555080056190491\n",
      "Epoch 1849: train loss: 0.7554758191108704\n",
      "Epoch 1850: train loss: 0.756048858165741\n",
      "Epoch 1851: train loss: 0.7553662061691284\n",
      "Epoch 1852: train loss: 0.7556601762771606\n",
      "Epoch 1853: train loss: 0.7556764483451843\n",
      "Epoch 1854: train loss: 0.756134569644928\n",
      "Epoch 1855: train loss: 0.7558103203773499\n",
      "Epoch 1856: train loss: 0.7553341388702393\n",
      "Epoch 1857: train loss: 0.7552312016487122\n",
      "Epoch 1858: train loss: 0.7551221251487732\n",
      "Epoch 1859: train loss: 0.7555965781211853\n",
      "Epoch 1860: train loss: 0.7553677558898926\n",
      "Epoch 1861: train loss: 0.7553232312202454\n",
      "Epoch 1862: train loss: 0.7553810477256775\n",
      "Epoch 1863: train loss: 0.7550904750823975\n",
      "Epoch 1864: train loss: 0.7550668120384216\n",
      "Epoch 1865: train loss: 0.7544907927513123\n",
      "Epoch 1866: train loss: 0.7545651197433472\n",
      "Epoch 1867: train loss: 0.75462806224823\n",
      "Epoch 1868: train loss: 0.7549000978469849\n",
      "Epoch 1869: train loss: 0.7546601295471191\n",
      "Epoch 1870: train loss: 0.7545849680900574\n",
      "Epoch 1871: train loss: 0.754384458065033\n",
      "Epoch 1872: train loss: 0.7543318271636963\n",
      "Epoch 1873: train loss: 0.7538980841636658\n",
      "Epoch 1874: train loss: 0.7548010349273682\n",
      "Epoch 1875: train loss: 0.7536346316337585\n",
      "Epoch 1876: train loss: 0.7539754509925842\n",
      "Epoch 1877: train loss: 0.7545682787895203\n",
      "Epoch 1878: train loss: 0.75440514087677\n",
      "Epoch 1879: train loss: 0.7542110085487366\n",
      "Epoch 1880: train loss: 0.7542569041252136\n",
      "Epoch 1881: train loss: 0.7539287209510803\n",
      "Epoch 1882: train loss: 0.7540600895881653\n",
      "Epoch 1883: train loss: 0.7543190121650696\n",
      "Epoch 1884: train loss: 0.754150390625\n",
      "Epoch 1885: train loss: 0.7539898753166199\n",
      "Epoch 1886: train loss: 0.7535722255706787\n",
      "Epoch 1887: train loss: 0.7537176012992859\n",
      "Epoch 1888: train loss: 0.7539986968040466\n",
      "Epoch 1889: train loss: 0.7540650963783264\n",
      "Epoch 1890: train loss: 0.7532417178153992\n",
      "Epoch 1891: train loss: 0.7543503046035767\n",
      "Epoch 1892: train loss: 0.753844678401947\n",
      "Epoch 1893: train loss: 0.7535815834999084\n",
      "Epoch 1894: train loss: 0.7532637119293213\n",
      "Epoch 1895: train loss: 0.7541194558143616\n",
      "Epoch 1896: train loss: 0.7533431053161621\n",
      "Epoch 1897: train loss: 0.753494143486023\n",
      "Epoch 1898: train loss: 0.7532545924186707\n",
      "Epoch 1899: train loss: 0.7534145712852478\n",
      "Epoch 1900: train loss: 0.7529045939445496\n",
      "Epoch 1901: train loss: 0.753669261932373\n",
      "Epoch 1902: train loss: 0.753030002117157\n",
      "Epoch 1903: train loss: 0.7527170777320862\n",
      "Epoch 1904: train loss: 0.7530806660652161\n",
      "Epoch 1905: train loss: 0.7528533339500427\n",
      "Epoch 1906: train loss: 0.7522754669189453\n",
      "Epoch 1907: train loss: 0.7531081438064575\n",
      "Epoch 1908: train loss: 0.7520517706871033\n",
      "Epoch 1909: train loss: 0.7522908449172974\n",
      "Epoch 1910: train loss: 0.7528635263442993\n",
      "Epoch 1911: train loss: 0.7526635527610779\n",
      "Epoch 1912: train loss: 0.752129077911377\n",
      "Epoch 1913: train loss: 0.7526575326919556\n",
      "Epoch 1914: train loss: 0.7518262267112732\n",
      "Epoch 1915: train loss: 0.7523051500320435\n",
      "Epoch 1916: train loss: 0.7526268362998962\n",
      "Epoch 1917: train loss: 0.7528072595596313\n",
      "Epoch 1918: train loss: 0.7524279356002808\n",
      "Epoch 1919: train loss: 0.7514577507972717\n",
      "Epoch 1920: train loss: 0.7523568868637085\n",
      "Epoch 1921: train loss: 0.752041757106781\n",
      "Epoch 1922: train loss: 0.7519502639770508\n",
      "Epoch 1923: train loss: 0.7521778345108032\n",
      "Epoch 1924: train loss: 0.7524119019508362\n",
      "Epoch 1925: train loss: 0.7522966265678406\n",
      "Epoch 1926: train loss: 0.7517329454421997\n",
      "Epoch 1927: train loss: 0.7514939904212952\n",
      "Epoch 1928: train loss: 0.7512184977531433\n",
      "Epoch 1929: train loss: 0.7516036033630371\n",
      "Epoch 1930: train loss: 0.751532793045044\n",
      "Epoch 1931: train loss: 0.7513764500617981\n",
      "Epoch 1932: train loss: 0.75168377161026\n",
      "Epoch 1933: train loss: 0.7509933114051819\n",
      "Epoch 1934: train loss: 0.7508420944213867\n",
      "Epoch 1935: train loss: 0.7511888146400452\n",
      "Epoch 1936: train loss: 0.7515277862548828\n",
      "Epoch 1937: train loss: 0.7511807680130005\n",
      "Epoch 1938: train loss: 0.7512309551239014\n",
      "Epoch 1939: train loss: 0.7509129047393799\n",
      "Epoch 1940: train loss: 0.7511395215988159\n",
      "Epoch 1941: train loss: 0.7505526542663574\n",
      "Epoch 1942: train loss: 0.7518952488899231\n",
      "Epoch 1943: train loss: 0.751011312007904\n",
      "Epoch 1944: train loss: 0.7507879137992859\n",
      "Epoch 1945: train loss: 0.7511784434318542\n",
      "Epoch 1946: train loss: 0.7515658736228943\n",
      "Epoch 1947: train loss: 0.7513198256492615\n",
      "Epoch 1948: train loss: 0.7511019110679626\n",
      "Epoch 1949: train loss: 0.751334547996521\n",
      "Epoch 1950: train loss: 0.7506351470947266\n",
      "Epoch 1951: train loss: 0.7511276602745056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1952: train loss: 0.7501840591430664\n",
      "Epoch 1953: train loss: 0.7500168681144714\n",
      "Epoch 1954: train loss: 0.751037061214447\n",
      "Epoch 1955: train loss: 0.7510544657707214\n",
      "Epoch 1956: train loss: 0.7509552240371704\n",
      "Epoch 1957: train loss: 0.7507730722427368\n",
      "Epoch 1958: train loss: 0.7504874467849731\n",
      "Epoch 1959: train loss: 0.7500722408294678\n",
      "Epoch 1960: train loss: 0.7498276829719543\n",
      "Epoch 1961: train loss: 0.7496412992477417\n",
      "Epoch 1962: train loss: 0.7503666281700134\n",
      "Epoch 1963: train loss: 0.7501276731491089\n",
      "Epoch 1964: train loss: 0.7495028972625732\n",
      "Epoch 1965: train loss: 0.7496060132980347\n",
      "Epoch 1966: train loss: 0.7500076293945312\n",
      "Epoch 1967: train loss: 0.7493695020675659\n",
      "Epoch 1968: train loss: 0.7506945133209229\n",
      "Epoch 1969: train loss: 0.7504408955574036\n",
      "Epoch 1970: train loss: 0.750298261642456\n",
      "Epoch 1971: train loss: 0.7497023940086365\n",
      "Epoch 1972: train loss: 0.7493782639503479\n",
      "Epoch 1973: train loss: 0.749546468257904\n",
      "Epoch 1974: train loss: 0.7496166229248047\n",
      "Epoch 1975: train loss: 0.7502033710479736\n",
      "Epoch 1976: train loss: 0.7489980459213257\n",
      "Epoch 1977: train loss: 0.7490066289901733\n",
      "Epoch 1978: train loss: 0.74940025806427\n",
      "Epoch 1979: train loss: 0.7491900324821472\n",
      "Epoch 1980: train loss: 0.7491452097892761\n",
      "Epoch 1981: train loss: 0.7490867376327515\n",
      "Epoch 1982: train loss: 0.7498554587364197\n",
      "Epoch 1983: train loss: 0.748685896396637\n",
      "Epoch 1984: train loss: 0.7494339942932129\n",
      "Epoch 1985: train loss: 0.7485907673835754\n",
      "Epoch 1986: train loss: 0.7480228543281555\n",
      "Epoch 1987: train loss: 0.7489134669303894\n",
      "Epoch 1988: train loss: 0.7487540245056152\n",
      "Epoch 1989: train loss: 0.7487574815750122\n",
      "Epoch 1990: train loss: 0.7483963966369629\n",
      "Epoch 1991: train loss: 0.7486395835876465\n",
      "Epoch 1992: train loss: 0.7486774921417236\n",
      "Epoch 1993: train loss: 0.7486729621887207\n",
      "Epoch 1994: train loss: 0.7480769157409668\n",
      "Epoch 1995: train loss: 0.7480107545852661\n",
      "Epoch 1996: train loss: 0.7481532096862793\n",
      "Epoch 1997: train loss: 0.7483725547790527\n",
      "Epoch 1998: train loss: 0.7476614713668823\n",
      "Epoch 1999: train loss: 0.7482284307479858\n"
     ]
    }
   ],
   "source": [
    "uin = torch.tensor(train.userIdCat.values).long().cuda()\n",
    "moin = torch.tensor(train.movieIdCat.values).long().cuda()\n",
    "out =  torch.tensor(train.rating.values).long().cuda()\n",
    "\n",
    "model = Network(n_movies, n_latent_factors_movie, n_users, n_latent_factors_user)\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay= 0.0001)\n",
    "model.train()\n",
    "epochs = 2000\n",
    "errors = []\n",
    "criterion = nn.L1Loss()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    y_pred = model(moin, uin)\n",
    "\n",
    "    loss = criterion(y_pred.squeeze().float(),out.float())\n",
    "    errors.append(loss.item())\n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"MDM-Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LoadModel = Network(n_movies, n_latent_factors_movie, n_users, n_latent_factors_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (embedding_m): Embedding(3707, 8)\n",
       "  (embedding_u): Embedding(6041, 5)\n",
       "  (lin1): Linear(in_features=13, out_features=200, bias=True)\n",
       "  (lin2): Linear(in_features=200, out_features=80, bias=True)\n",
       "  (lin3): Linear(in_features=80, out_features=50, bias=True)\n",
       "  (lin4): Linear(in_features=50, out_features=20, bias=True)\n",
       "  (lin5): Linear(in_features=20, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LoadModel.load_state_dict(torch.load(\"MDM-Model\"))\n",
    "LoadModel.cuda()\n",
    "LoadModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_m = torch.tensor(test.movieIdCat.values).long().to('cuda')\n",
    "test_u = torch.tensor(test.userIdCat.values).long().to('cuda')\n",
    "test_pred = LoadModel(test_m, test_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.4383],\n",
       "        [3.9978],\n",
       "        [4.3658],\n",
       "        ...,\n",
       "        [4.2153],\n",
       "        [3.8964],\n",
       "        [3.8037]], grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "test_pred.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalResults=test_pred.squeeze().to('cpu').detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted_rating'] = FinalResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7039677480353554\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(test['rating'], test['predicted_rating']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904966466712323\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(test['predicted_rating'], test['rating']))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = test[['userId', 'movieId', 'predicted_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading movies data and creating movies dictionary\n",
    "movies_df = pd.read_csv('movies.csv', usecols=[0,1])\n",
    "movies_dict = pd.Series(movies_df['title'].values,index=movies_df['movieId']).to_dict()\n",
    "users_list = pred_df['userId'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/koko/system/anaconda/envs/python37/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# recmmendations from ratings predicted by Neural Network\n",
    "pred_df['predicted_movies'] = pred_df.apply(lambda x: (x['movieId'], x['predicted_rating']), axis=1)\n",
    "pred_rec_df = pred_df[['userId', 'predicted_movies']].groupby('userId')['predicted_movies'].apply(list).reset_index(name='recommendation')\n",
    "pred_rec_df['recommendation'] = pred_rec_df['recommendation'].apply(lambda x: sorted(x, key=lambda tup: tup[1], reverse=True))\n",
    "sorted_recs_dict = pd.Series(pred_rec_df['recommendation'].values,index=pred_rec_df['userId']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/koko/system/anaconda/envs/python37/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# recmmendations from ratings in dataset\n",
    "pred_df_t = test[['userId', 'movieId', 'rating']]\n",
    "pred_df_t['predicted_movies'] = pred_df_t.apply(lambda x: (x['movieId'], x['rating']), axis=1)\n",
    "pred_rec_df_t = pred_df_t[['userId', 'predicted_movies']].groupby('userId')['predicted_movies'].apply(list).reset_index(name='recommendation')\n",
    "pred_rec_df_t['recommendation'] = pred_rec_df_t['recommendation'].apply(lambda x: sorted(x, key=lambda tup: tup[1], reverse=True))\n",
    "sorted_recs_dict_t = pd.Series(pred_rec_df_t['recommendation'].values,index=pred_rec_df_t['userId']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recommendation function\n",
    "def n_rec(user, n, p):\n",
    "    if p==True:\n",
    "        n_recs = sorted_recs_dict[user][:n]\n",
    "    else:\n",
    "        n_recs = sorted_recs_dict_t[user][:n]\n",
    "    return [movies_dict[int(x[0])] for x in n_recs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6374516663597865 0.6374516663597865 0.6374516663597865\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "for user in users_list:\n",
    "    pred_recs = n_rec(user=user,n=10,p=True)\n",
    "    rat_recs = n_rec(user=user,n=10,p=False)\n",
    "    tp = tp + len(list(set(pred_recs) & set(rat_recs)))\n",
    "    fp = fp + len(list(set(pred_recs) - set(rat_recs)))\n",
    "    fn = fn + len(list(set(rat_recs) - set(pred_recs)))\n",
    "precision = tp/float(fp+tp)\n",
    "recall = tp/float(fn+tp)\n",
    "fscore = (2*precision*recall)/(precision+recall)\n",
    "\n",
    "print(precision,recall, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
